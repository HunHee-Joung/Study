"""
ì§„ì§œ LangGraph ê¸°ë°˜ ì¸í…”ë¦¬ì „íŠ¸ ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡  ì‹œìŠ¤í…œ
ìƒíƒœ ê¸°ë°˜ ê·¸ë˜í”„ ì›Œí¬í”Œë¡œìš°ë¡œ êµ¬í˜„ëœ ì™„ì „í•œ AI í† ë¡  ì‹œìŠ¤í…œ
"""

import os
import json
import time
import re
import operator
from datetime import datetime
from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Optional, Annotated, Literal

# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ import
try:
    from openai import OpenAI
    from langchain_openai import ChatOpenAI
    from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
    from langchain_core.prompts import ChatPromptTemplate
    from langgraph.graph import StateGraph, END
    from langgraph.prebuilt import ToolNode
    from langgraph.checkpoint.memory import MemorySaver
except ImportError as e:
    print(f"âŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {e}")
    print("ì„¤ì¹˜ ëª…ë ¹ì–´:")
    print("pip install openai langchain-openai langgraph")
    exit(1)

# ========================
# ìƒíƒœ ì •ì˜ (LangGraph í•µì‹¬)
# ========================

@dataclass
class DebateMessage:
    """í† ë¡  ë©”ì‹œì§€"""
    speaker: str
    content: str
    timestamp: datetime
    round_num: int
    tokens_used: int = 0
    agent_type: str = ""

class DebateState(dict):
    """LangGraphìš© ìƒíƒœ í´ë˜ìŠ¤ (TypedDict ìŠ¤íƒ€ì¼)"""
    messages: Annotated[List[DebateMessage], operator.add]
    topic: str
    current_speaker: str
    round_count: int
    max_rounds: int
    is_finished: bool
    topic_analysis: Optional[Dict]
    total_tokens: int
    moderator_analysis: Optional[str]

@dataclass
class AgentProfile:
    """ì—ì´ì „íŠ¸ í”„ë¡œí•„"""
    name: str
    role: str
    position: str
    expertise: List[str]
    key_arguments: List[str]
    response_style: str

@dataclass 
class TopicAnalysis:
    """ì£¼ì œ ë¶„ì„ ê²°ê³¼"""
    topic: str
    category: str
    stakeholders: List[str]
    key_issues: List[str]
    pro_agent: AgentProfile
    con_agent: AgentProfile

# ========================
# ì„¤ì • í´ë˜ìŠ¤
# ========================

@dataclass
class LangGraphConfig:
    """LangGraph ì„¤ì •"""
    openai_api_key: str
    model: str = "gpt-4o-mini"
    temperature: float = 0.7
    max_tokens: int = 500
    max_rounds: int = 3
    enable_checkpoints: bool = True
    save_to_file: bool = True

# ========================
# ì£¼ì œ ë¶„ì„ ì‹œìŠ¤í…œ
# ========================

class LangGraphTopicAnalyzer:
    """LangGraphìš© ì£¼ì œ ë¶„ì„ê¸°"""
    
    def __init__(self, config: LangGraphConfig):
        self.config = config
        self.llm = ChatOpenAI(
            api_key=config.openai_api_key,
            model=config.model,
            temperature=0.5
        )
    
    def analyze_topic(self, topic: str) -> TopicAnalysis:
        """ì£¼ì œ ë¶„ì„ ë° ì—ì´ì „íŠ¸ í”„ë¡œí•„ ìƒì„±"""
        print(f"ğŸ§  LangGraph ì£¼ì œ ë¶„ì„ ì¤‘: '{topic}'")
        
        analysis_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ í† ë¡  ì£¼ì œë¥¼ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” AIì…ë‹ˆë‹¤. 
            ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•´ ì°¬ë°˜ ì–‘ì¸¡ì˜ ì´í•´ê´€ê³„ìì™€ í•µì‹¬ ë…¼ì ì„ ë¶„ì„í•©ë‹ˆë‹¤."""),
            ("user", """ë‹¤ìŒ í† ë¡  ì£¼ì œë¥¼ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”: "{topic}"

ì‘ë‹µ í˜•ì‹:
{{
  "category": "ì •ì±…/ì‚¬íšŒ/ê²½ì œ/ê¸°ìˆ /í™˜ê²½/êµìœ¡/ì˜ë£Œ/ê¸ˆìœµ ì¤‘ ì„ íƒ",
  "key_issues": ["ìŸì 1", "ìŸì 2", "ìŸì 3", "ìŸì 4"],
  "pro_stakeholder": "ì°¬ì„±ì¸¡ ì´í•´ê´€ê³„ì (êµ¬ì²´ì  ê¸°ê´€/ë‹¨ì²´ëª…)",
  "con_stakeholder": "ë°˜ëŒ€ì¸¡ ì´í•´ê´€ê³„ì (êµ¬ì²´ì  ê¸°ê´€/ë‹¨ì²´ëª…)",
  "pro_arguments": ["ì°¬ì„± ë…¼ê±°1", "ì°¬ì„± ë…¼ê±°2", "ì°¬ì„± ë…¼ê±°3"],
  "con_arguments": ["ë°˜ëŒ€ ë…¼ê±°1", "ë°˜ëŒ€ ë…¼ê±°2", "ë°˜ëŒ€ ë…¼ê±°3"]
}}

í•œêµ­ì–´ë¡œ êµ¬ì²´ì ì´ê³  í˜„ì‹¤ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.""")
        ])
        
        try:
            chain = analysis_prompt | self.llm
            response = chain.invoke({"topic": topic})
            
            # JSON íŒŒì‹±
            content = response.content
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if json_match:
                analysis_data = json.loads(json_match.group())
            else:
                raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ì—ì´ì „íŠ¸ í”„ë¡œí•„ ìƒì„±
            pro_agent = AgentProfile(
                name=analysis_data.get("pro_stakeholder", "ì°¬ì„± ì „ë¬¸ê°€"),
                role=f"{analysis_data.get('category', 'ì •ì±…')} ì „ë¬¸ê°€",
                position="ì°¬ì„±",
                expertise=[analysis_data.get('category', 'ì •ì±…'), "ì •ì±… ë¶„ì„"],
                key_arguments=analysis_data.get("pro_arguments", []),
                response_style="ì ê·¹ì ì´ê³  ë¯¸ë˜ì§€í–¥ì "
            )
            
            con_agent = AgentProfile(
                name=analysis_data.get("con_stakeholder", "ë°˜ëŒ€ ì „ë¬¸ê°€"),
                role=f"{analysis_data.get('category', 'ì •ì±…')} ì „ë¬¸ê°€",
                position="ë°˜ëŒ€", 
                expertise=[analysis_data.get('category', 'ì •ì±…'), "ìœ„í—˜ ë¶„ì„"],
                key_arguments=analysis_data.get("con_arguments", []),
                response_style="ì‹ ì¤‘í•˜ê³  í˜„ì‹¤ì "
            )
            
            return TopicAnalysis(
                topic=topic,
                category=analysis_data.get("category", "ì •ì±…"),
                stakeholders=[pro_agent.name, con_agent.name],
                key_issues=analysis_data.get("key_issues", []),
                pro_agent=pro_agent,
                con_agent=con_agent
            )
            
        except Exception as e:
            print(f"âš ï¸ ì£¼ì œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._create_fallback_analysis(topic)
    
    def _create_fallback_analysis(self, topic: str) -> TopicAnalysis:
        """ë¶„ì„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì—ì´ì „íŠ¸ ìƒì„±"""
        pro_agent = AgentProfile(
            name="ì°¬ì„± ì „ë¬¸ê°€",
            role="ì •ì±… ì „ë¬¸ê°€",
            position="ì°¬ì„±",
            expertise=["ì •ì±… ë¶„ì„"],
            key_arguments=["í•„ìš”ì„± ì¸ì •", "ê¸ì •ì  íš¨ê³¼"],
            response_style="ì ê·¹ì "
        )
        
        con_agent = AgentProfile(
            name="ë°˜ëŒ€ ì „ë¬¸ê°€",
            role="ì •ì±… ì „ë¬¸ê°€", 
            position="ë°˜ëŒ€",
            expertise=["ìœ„í—˜ ë¶„ì„"],
            key_arguments=["ë¶€ì‘ìš© ìš°ë ¤", "í˜„ì‹¤ì  í•œê³„"],
            response_style="ì‹ ì¤‘í•¨"
        )
        
        return TopicAnalysis(
            topic=topic,
            category="ì •ì±…",
            stakeholders=["ì°¬ì„± ì „ë¬¸ê°€", "ë°˜ëŒ€ ì „ë¬¸ê°€"],
            key_issues=["í•„ìš”ì„±", "ì‹¤í˜„ê°€ëŠ¥ì„±", "ë¶€ì‘ìš©", "ëŒ€ì•ˆ"],
            pro_agent=pro_agent,
            con_agent=con_agent
        )

# ========================
# LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤
# ========================

def pro_agent_node(state: DebateState) -> DebateState:
    """ì°¬ì„± ì—ì´ì „íŠ¸ ë…¸ë“œ"""
    print(f"\nâœ… **{state['topic_analysis']['pro_agent']['name']}** ë°œì–¸ ì¤‘...")
    
    # LLM ì´ˆê¸°í™”
    llm = ChatOpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        model="gpt-4o-mini",
        temperature=0.7
    )
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    agent_info = state["topic_analysis"]["pro_agent"]
    system_prompt = f"""ë‹¹ì‹ ì€ '{agent_info["name"]}'ì„ ëŒ€ë³€í•˜ëŠ” {agent_info["role"]}ì…ë‹ˆë‹¤.

í† ë¡  ì£¼ì œ: {state["topic"]}
ë‹¹ì‹ ì˜ ì…ì¥: {agent_info["position"]}

í•µì‹¬ ì£¼ì¥:
{chr(10).join(f"- {arg}" for arg in agent_info["key_arguments"])}

ì‘ë‹µ ìŠ¤íƒ€ì¼: {agent_info["response_style"]}

ì§€ì¹¨:
1. ë‹¹ì‹ ì˜ ì…ì¥ì„ ëª…í™•í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ì£¼ì¥í•˜ì„¸ìš”
2. êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ ì‚¬ë¡€ë¥¼ ì œì‹œí•˜ì„¸ìš”
3. ìƒëŒ€ë°© ì£¼ì¥ì— ëŒ€í•´ ë…¼ë¦¬ì ìœ¼ë¡œ ë°˜ë°•í•˜ì„¸ìš”
4. 300ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”
5. í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”"""
    
    # ëŒ€í™” ë§¥ë½ êµ¬ì„±
    context = _build_conversation_context(state["messages"], state["round_count"])
    
    user_message = f"""
í† ë¡  ì£¼ì œ: {state["topic"]}
í˜„ì¬ ë¼ìš´ë“œ: {state["round_count"] + 1}

ì´ì „ ëŒ€í™”:
{context}

ìœ„ ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ {agent_info["name"]} ì…ì¥ì—ì„œ ë…¼ë¦¬ì ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""
    
    # AI ì‘ë‹µ ìƒì„±
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_message)
    ]
    
    try:
        start_time = time.time()
        response = llm.invoke(messages)
        response_time = time.time() - start_time
        
        # ë©”ì‹œì§€ ìƒì„±
        debate_message = DebateMessage(
            speaker=agent_info["name"],
            content=response.content.strip(),
            timestamp=datetime.now(),
            round_num=state["round_count"] + 1,
            tokens_used=getattr(response, 'usage', {}).get('total_tokens', 0),
            agent_type="pro"
        )
        
        print(f"   ğŸ’¬ {response.content.strip()}")
        print(f"   â±ï¸ ì‘ë‹µ ì‹œê°„: {response_time:.1f}ì´ˆ")
        
        return {
            "messages": [debate_message],
            "current_speaker": "con",
            "total_tokens": state.get("total_tokens", 0) + debate_message.tokens_used
        }
        
    except Exception as e:
        error_message = DebateMessage(
            speaker=agent_info["name"],
            content=f"[ì˜¤ë¥˜] ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}",
            timestamp=datetime.now(),
            round_num=state["round_count"] + 1,
            agent_type="pro"
        )
        
        return {
            "messages": [error_message],
            "current_speaker": "con",
            "total_tokens": state.get("total_tokens", 0)
        }

def con_agent_node(state: DebateState) -> DebateState:
    """ë°˜ëŒ€ ì—ì´ì „íŠ¸ ë…¸ë“œ"""
    print(f"\nâŒ **{state['topic_analysis']['con_agent']['name']}** ë°œì–¸ ì¤‘...")
    
    # LLM ì´ˆê¸°í™”
    llm = ChatOpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        model="gpt-4o-mini",
        temperature=0.7
    )
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    agent_info = state["topic_analysis"]["con_agent"]
    system_prompt = f"""ë‹¹ì‹ ì€ '{agent_info["name"]}'ì„ ëŒ€ë³€í•˜ëŠ” {agent_info["role"]}ì…ë‹ˆë‹¤.

í† ë¡  ì£¼ì œ: {state["topic"]}
ë‹¹ì‹ ì˜ ì…ì¥: {agent_info["position"]}

í•µì‹¬ ì£¼ì¥:
{chr(10).join(f"- {arg}" for arg in agent_info["key_arguments"])}

ì‘ë‹µ ìŠ¤íƒ€ì¼: {agent_info["response_style"]}

ì§€ì¹¨:
1. ë‹¹ì‹ ì˜ ì…ì¥ì„ ëª…í™•í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ì£¼ì¥í•˜ì„¸ìš”
2. êµ¬ì²´ì ì¸ ê·¼ê±°ì™€ ì‚¬ë¡€ë¥¼ ì œì‹œí•˜ì„¸ìš”
3. ìƒëŒ€ë°© ì£¼ì¥ì— ëŒ€í•´ ë…¼ë¦¬ì ìœ¼ë¡œ ë°˜ë°•í•˜ì„¸ìš”
4. 300ì ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”
5. í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ì„¸ìš”"""
    
    # ëŒ€í™” ë§¥ë½ êµ¬ì„±
    context = _build_conversation_context(state["messages"], state["round_count"])
    
    user_message = f"""
í† ë¡  ì£¼ì œ: {state["topic"]}
í˜„ì¬ ë¼ìš´ë“œ: {state["round_count"] + 1}

ì´ì „ ëŒ€í™”:
{context}

ìœ„ ë§¥ë½ì„ ë°”íƒ•ìœ¼ë¡œ {agent_info["name"]} ì…ì¥ì—ì„œ ë…¼ë¦¬ì ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""
    
    # AI ì‘ë‹µ ìƒì„±
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_message)
    ]
    
    try:
        start_time = time.time()
        response = llm.invoke(messages)
        response_time = time.time() - start_time
        
        # ë©”ì‹œì§€ ìƒì„±
        debate_message = DebateMessage(
            speaker=agent_info["name"],
            content=response.content.strip(),
            timestamp=datetime.now(),
            round_num=state["round_count"] + 1,
            tokens_used=getattr(response, 'usage', {}).get('total_tokens', 0),
            agent_type="con"
        )
        
        print(f"   ğŸ’¬ {response.content.strip()}")
        print(f"   â±ï¸ ì‘ë‹µ ì‹œê°„: {response_time:.1f}ì´ˆ")
        
        return {
            "messages": [debate_message],
            "current_speaker": "moderator",
            "total_tokens": state.get("total_tokens", 0) + debate_message.tokens_used
        }
        
    except Exception as e:
        error_message = DebateMessage(
            speaker=agent_info["name"],
            content=f"[ì˜¤ë¥˜] ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {str(e)}",
            timestamp=datetime.now(),
            round_num=state["round_count"] + 1,
            agent_type="con"
        )
        
        return {
            "messages": [error_message],
            "current_speaker": "moderator",
            "total_tokens": state.get("total_tokens", 0)
        }

def moderator_node(state: DebateState) -> DebateState:
    """ì‚¬íšŒì ë…¸ë“œ - ë¼ìš´ë“œ ê´€ë¦¬ ë° íë¦„ ì œì–´"""
    print(f"\nâš–ï¸ **ì‚¬íšŒì** ë¼ìš´ë“œ {state['round_count'] + 1} ì§„í–‰ ì¤‘...")
    
    # ë¼ìš´ë“œ ì¦ê°€
    new_round_count = state["round_count"] + 1
    
    # í† ë¡  ì¢…ë£Œ ì¡°ê±´ í™•ì¸
    if new_round_count >= state["max_rounds"]:
        print(f"   ğŸ“¢ {state['max_rounds']}ë¼ìš´ë“œ ì™„ë£Œ! í† ë¡ ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return {
            "round_count": new_round_count,
            "is_finished": True,
            "current_speaker": "end"
        }
    else:
        print(f"   ğŸ“¢ ë¼ìš´ë“œ {new_round_count} ì™„ë£Œ. ë‹¤ìŒ ë¼ìš´ë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        return {
            "round_count": new_round_count,
            "current_speaker": "pro",  # ë‹¤ìŒ ë¼ìš´ë“œëŠ” ì°¬ì„±ì¸¡ë¶€í„°
            "is_finished": False
        }

def final_analysis_node(state: DebateState) -> DebateState:
    """ìµœì¢… ë¶„ì„ ë…¸ë“œ"""
    print(f"\nğŸ¯ **AI ìµœì¢… ë¶„ì„** ìƒì„± ì¤‘...")
    
    # LLM ì´ˆê¸°í™”
    llm = ChatOpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        model="gpt-4o-mini",
        temperature=0.3  # ë¶„ì„ì€ ë” ê°ê´€ì ìœ¼ë¡œ
    )
    
    # í† ë¡  ë‚´ìš© ì •ë¦¬
    conversation_summary = "\n".join([
        f"[{msg.speaker}]: {msg.content}" for msg in state["messages"]
    ])
    
    analysis_prompt = f"""
ë‹¤ìŒì€ '{state["topic"]}'ì— ëŒ€í•œ í† ë¡  ë‚´ìš©ì…ë‹ˆë‹¤.

ì°¸ì—¬ì:
- {state["topic_analysis"]["pro_agent"]["name"]} (ì°¬ì„±)
- {state["topic_analysis"]["con_agent"]["name"]} (ë°˜ëŒ€)

í† ë¡  ë‚´ìš©:
{conversation_summary}

ìœ„ í† ë¡ ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ í¬í•¨í•œ ê°ê´€ì  í‰ê°€ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

1. **í•µì‹¬ ìŸì  ì •ë¦¬**: ì–‘ì¸¡ì´ ì§‘ì¤‘í•œ ì£¼ìš” ë…¼ì ë“¤
2. **ë…¼ë¦¬ ê°•ë„ í‰ê°€**: ê° ì¸¡ ì£¼ì¥ì˜ ì„¤ë“ë ¥ê³¼ ê·¼ê±°
3. **í˜„ì‹¤ì„± ê²€í† **: ì œì‹œëœ ë°©ì•ˆë“¤ì˜ ì‹¤í˜„ ê°€ëŠ¥ì„±
4. **ê· í˜•ì  ëª¨ìƒ‰**: ì–‘ì¸¡ì„ ì•„ìš°ë¥´ëŠ” í•´ê²° ë°©í–¥
5. **ì¢…í•© í‰ê°€**: í† ë¡ ì˜ ì „ë°˜ì  ì§ˆê³¼ ì‹œì‚¬ì 

500ì ë‚´ì™¸ë¡œ ì „ë¬¸ì ì´ê³  ê· í˜• ì¡íŒ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
    
    try:
        response = llm.invoke([HumanMessage(content=analysis_prompt)])
        analysis_content = response.content.strip()
        
        print(f"   ğŸ¯ {analysis_content}")
        
        return {
            "moderator_analysis": analysis_content,
            "total_tokens": state.get("total_tokens", 0) + getattr(response, 'usage', {}).get('total_tokens', 0)
        }
        
    except Exception as e:
        error_analysis = f"[ë¶„ì„ ì˜¤ë¥˜] ìµœì¢… ë¶„ì„ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}"
        print(f"   âš ï¸ {error_analysis}")
        
        return {
            "moderator_analysis": error_analysis,
            "total_tokens": state.get("total_tokens", 0)
        }

# ========================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ========================

def _build_conversation_context(messages: List[DebateMessage], round_count: int) -> str:
    """ëŒ€í™” ë§¥ë½ êµ¬ì„±"""
    if not messages:
        return "í† ë¡  ì‹œì‘ - ì²« ë°œì–¸"
    
    # ìµœê·¼ 6ê°œ ë©”ì‹œì§€ë§Œ ì‚¬ìš©
    recent_messages = messages[-6:] if len(messages) > 6 else messages
    context_parts = []
    
    for msg in recent_messages:
        context_parts.append(f"[Round {msg.round_num}] {msg.speaker}: {msg.content}")
    
    return "\n".join(context_parts)

# ========================
# ë¼ìš°íŒ… í•¨ìˆ˜ë“¤ (LangGraph í•µì‹¬)
# ========================

def should_continue(state: DebateState) -> Literal["pro", "con", "moderator", "final_analysis", "end"]:
    """ë‹¤ìŒ ë…¸ë“œ ê²°ì • - LangGraphì˜ ì¡°ê±´ë¶€ ë¼ìš°íŒ…"""
    
    if state.get("is_finished", False):
        # í† ë¡ ì´ ëë‚¬ìœ¼ë©´ ìµœì¢… ë¶„ì„ìœ¼ë¡œ
        if state.get("moderator_analysis") is None:
            return "final_analysis"
        else:
            return "end"
    
    current_speaker = state.get("current_speaker", "pro")
    
    if current_speaker == "pro":
        return "pro"
    elif current_speaker == "con": 
        return "con"
    elif current_speaker == "moderator":
        return "moderator"
    else:
        return "end"

# ========================
# LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±
# ========================

def create_langgraph_debate_workflow() -> StateGraph:
    """LangGraph í† ë¡  ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    
    # StateGraph ì´ˆê¸°í™”
    workflow = StateGraph(DebateState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("pro_agent", pro_agent_node)
    workflow.add_node("con_agent", con_agent_node)
    workflow.add_node("moderator", moderator_node)
    workflow.add_node("final_analysis", final_analysis_node)
    
    # ì‹œì‘ì  ì„¤ì •
    workflow.set_entry_point("pro_agent")
    
    # ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€ (LangGraphì˜ í•µì‹¬ ê¸°ëŠ¥)
    workflow.add_conditional_edges(
        "pro_agent",
        should_continue,
        {
            "con": "con_agent",
            "end": END
        }
    )
    
    workflow.add_conditional_edges(
        "con_agent", 
        should_continue,
        {
            "moderator": "moderator",
            "end": END
        }
    )
    
    workflow.add_conditional_edges(
        "moderator",
        should_continue,
        {
            "pro": "pro_agent",
            "final_analysis": "final_analysis",
            "end": END
        }
    )
    
    workflow.add_conditional_edges(
        "final_analysis",
        should_continue,
        {
            "end": END
        }
    )
    
    return workflow

# ========================
# ë©”ì¸ LangGraph í† ë¡  ì‹œìŠ¤í…œ
# ========================

class LangGraphDebateSystem:
    """LangGraph ê¸°ë°˜ í† ë¡  ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: LangGraphConfig):
        self.config = config
        self.topic_analyzer = LangGraphTopicAnalyzer(config)
        
        # ì²´í¬í¬ì¸íŠ¸ ì„¤ì • (ì„ íƒì‚¬í•­)
        self.memory = MemorySaver() if config.enable_checkpoints else None
        
        # ì›Œí¬í”Œë¡œìš° ìƒì„±
        self.workflow = create_langgraph_debate_workflow()
        
        # ì•± ì»´íŒŒì¼ (ì²´í¬í¬ì¸íŠ¸ í¬í•¨)
        if self.memory:
            self.app = self.workflow.compile(checkpointer=self.memory)
        else:
            self.app = self.workflow.compile()
    
    def run_debate(self, topic: str) -> Dict:
        """LangGraphë¡œ í† ë¡  ì‹¤í–‰"""
        print("=" * 80)
        print("ğŸ§  **LangGraph ê¸°ë°˜ ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡  ì‹œìŠ¤í…œ**")
        print("=" * 80)
        print(f"ğŸ“‹ **í† ë¡  ì£¼ì œ**: {topic}")
        print("=" * 80)
        
        # 1. ì£¼ì œ ë¶„ì„
        topic_analysis = self.topic_analyzer.analyze_topic(topic)
        self._print_analysis_results(topic_analysis)
        
        # 2. ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state = {
            "messages": [],
            "topic": topic,
            "current_speaker": "pro",
            "round_count": 0,
            "max_rounds": self.config.max_rounds,
            "is_finished": False,
            "topic_analysis": {
                "pro_agent": {
                    "name": topic_analysis.pro_agent.name,
                    "role": topic_analysis.pro_agent.role,
                    "position": topic_analysis.pro_agent.position,
                    "key_arguments": topic_analysis.pro_agent.key_arguments,
                    "response_style": topic_analysis.pro_agent.response_style
                },
                "con_agent": {
                    "name": topic_analysis.con_agent.name,
                    "role": topic_analysis.con_agent.role,
                    "position": topic_analysis.con_agent.position,
                    "key_arguments": topic_analysis.con_agent.key_arguments,
                    "response_style": topic_analysis.con_agent.response_style
                }
            },
            "total_tokens": 0,
            "moderator_analysis": None
        }
        
        # 3. LangGraph ì‹¤í–‰
        try:
            print(f"\nğŸš€ LangGraph ì›Œí¬í”Œë¡œìš° ì‹œì‘...")
            
            # ì²´í¬í¬ì¸íŠ¸ ì„¤ì •
            config = {"configurable": {"thread_id": f"debate_{int(time.time())}"}} if self.memory else None
            
            final_state = None
            step_count = 0
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            for step in self.app.stream(initial_state, config):
                step_count += 1
                print(f"\nğŸ”„ **Step {step_count}**: {list(step.keys())[0]}")
                print("-" * 50)
                
                # ìƒíƒœ ì—…ë°ì´íŠ¸
                for node_name, node_output in step.items():
                    if node_output:
                        final_state = node_output
                
                # ë¬´í•œ ë£¨í”„ ë°©ì§€
                if step_count > 20:
                    print("âš ï¸ ìµœëŒ€ ì‹¤í–‰ íšŸìˆ˜ ë„ë‹¬")
                    break
            
            return self._finalize_debate(final_state or initial_state, topic_analysis)
            
        except Exception as e:
            print(f"âŒ LangGraph ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return {"error": str(e)}
    
    def _print_analysis_results(self, topic_analysis: TopicAnalysis):
        """ì£¼ì œ ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print(f"\nğŸ¯ **ì£¼ì œ ë¶„ì„ ì™„ë£Œ**")
        print("=" * 50)
        print(f"ğŸ“‚ ë¶„ì•¼: {topic_analysis.category}")
        print(f"âœ… ì°¬ì„±ì¸¡: {topic_analysis.pro_agent.name}")
        print(f"âŒ ë°˜ëŒ€ì¸¡: {topic_analysis.con_agent.name}")
        print(f"\nğŸ” í•µì‹¬ ìŸì :")
        for i, issue in enumerate(topic_analysis.key_issues, 1):
            print(f"   {i}. {issue}")
        print("=" * 50)
    
    def _finalize_debate(self, final_state: Dict, topic_analysis: TopicAnalysis) -> Dict:
        """í† ë¡  ë§ˆë¬´ë¦¬"""
        print(f"\n" + "=" * 80)
        print("ğŸ **LangGraph í† ë¡  ì™„ë£Œ**")
        print("=" * 80)
        
        # í†µê³„ ì¶œë ¥
        messages = final_state.get("messages", [])
        total_tokens = final_state.get("total_tokens", 0)
        
        pro_messages = [m for m in messages if m.agent_type == "pro"]
        con_messages = [m for m in messages if m.agent_type == "con"]
        
        print(f"ğŸ“Š **ìµœì¢… í†µê³„**:")
        print(f"   â€¢ ì™„ë£Œ ë¼ìš´ë“œ: {final_state.get('round_count', 0)}/{self.config.max_rounds}")
        print(f"   â€¢ ì´ ë©”ì‹œì§€: {len(messages)}ê°œ")
        print(f"   â€¢ ì´ í† í°: {total_tokens:,} tokens")
        print(f"   â€¢ ì˜ˆìƒ ë¹„ìš©: ${total_tokens * 0.00015:.4f}")
        
        print(f"\nğŸ“ˆ **ì°¸ì—¬ìë³„ í†µê³„**:")
        print(f"âœ… {topic_analysis.pro_agent.name}: {len(pro_messages)}íšŒ ë°œì–¸")
        print(f"âŒ {topic_analysis.con_agent.name}: {len(con_messages)}íšŒ ë°œì–¸")
        
        if final_state.get("moderator_analysis"):
            print(f"\nğŸ¯ **AI ìµœì¢… ë¶„ì„**:")
            print(f"   {final_state['moderator_analysis']}")
        
        print("=" * 80)
        
        # ë¡œê·¸ ì €ì¥
        if self.config.save_to_file:
            self._save_langgraph_log(final_state, topic_analysis)
        
        return final_state
    
    def _save_langgraph_log(self, final_state: Dict, topic_analysis: TopicAnalysis):
        """LangGraph í† ë¡  ë¡œê·¸ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_topic = re.sub(r'[^\w\s-]', '', final_state["topic"].replace(' ', '_'))[:30]
            filename = f"langgraph_debate_{safe_topic}_{timestamp}.json"
            
            # ë©”ì‹œì§€ ì§ë ¬í™”
            serialized_messages = []
            for msg in final_state.get("messages", []):
                serialized_messages.append({
                    "speaker": msg.speaker,
                    "content": msg.content,
                    "timestamp": msg.timestamp.isoformat(),
                    "round_num": msg.round_num,
                    "tokens_used": msg.tokens_used,
                    "agent_type": msg.agent_type
                })
            
            log_data = {
                "metadata": {
                    "system": "LangGraph Multi-Agent Debate",
                    "topic": final_state["topic"],
                    "timestamp": timestamp,
                    "model": self.config.model,
                    "rounds": final_state.get("round_count", 0),
                    "max_rounds": self.config.max_rounds,
                    "checkpoints_enabled": self.config.enable_checkpoints
                },
                "topic_analysis": {
                    "category": topic_analysis.category,
                    "key_issues": topic_analysis.key_issues,
                    "pro_agent": {
                        "name": topic_analysis.pro_agent.name,
                        "role": topic_analysis.pro_agent.role,
                        "key_arguments": topic_analysis.pro_agent.key_arguments
                    },
                    "con_agent": {
                        "name": topic_analysis.con_agent.name,
                        "role": topic_analysis.con_agent.role,
                        "key_arguments": topic_analysis.con_agent.key_arguments
                    }
                },
                "workflow_execution": {
                    "total_tokens": final_state.get("total_tokens", 0),
                    "estimated_cost": final_state.get("total_tokens", 0) * 0.00015,
                    "is_finished": final_state.get("is_finished", False),
                    "moderator_analysis": final_state.get("moderator_analysis")
                },
                "conversation": serialized_messages
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ LangGraph í† ë¡  ë¡œê·¸ ì €ì¥: {filename}")
            
        except Exception as e:
            print(f"âš ï¸ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {e}")

# ========================
# ì£¼ì œ ì¶”ì²œ ì‹œìŠ¤í…œ (ê¸°ì¡´ê³¼ ë™ì¼)
# ========================

class LangGraphTopicRecommender:
    """LangGraphìš© ì£¼ì œ ì¶”ì²œ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.topic_categories = {
            "ì •ì±…": [
                "ê¸°ë³¸ì†Œë“ ë„ì…ì˜ í•„ìš”ì„±",
                "êµ­ë°©ë¹„ ì¦ì•¡ì€ í•„ìš”í•œê°€",
                "ì›ìë ¥ ë°œì „ í™•ëŒ€ vs ì¬ìƒì—ë„ˆì§€ ì „í™˜",
                "ë¶€ë™ì‚° ë³´ìœ ì„¸ ê°•í™” ì •ì±…",
                "ìµœì €ì„ê¸ˆ ëŒ€í­ ì¸ìƒì˜ íš¨ê³¼"
            ],
            "ì‚¬íšŒ": [
                "ì‚¬í˜•ì œ íì§€ vs ìœ ì§€",
                "ë™ì„±í˜¼ í•©ë²•í™” ë…¼ë€",
                "ì²­ì†Œë…„ ê²Œì„ ì‹œê°„ ì œí•œ ì •ì±…",
                "ë‚œë¯¼ ìˆ˜ìš© í™•ëŒ€ ì •ì±…",
                "ì¢…êµ ì‹œì„¤ ì„¸ê¸ˆ ë©´ì œ íì§€"
            ],
            "ê¸°ìˆ ": [
                "AI ê°œë°œ ê·œì œì˜ í•„ìš”ì„±", 
                "ììœ¨ì£¼í–‰ì°¨ ìƒìš©í™” ì‹œê¸°",
                "ë©”íƒ€ë²„ìŠ¤ êµìœ¡ ë„ì…",
                "ì•”í˜¸í™”í ì „ë©´ ê¸ˆì§€ vs í—ˆìš©",
                "ë¡œë´‡ì„¸ ë„ì… ë…¼ì˜"
            ],
            "êµìœ¡": [
                "ëŒ€í•™ ì…ì‹œì œë„ ì „ë©´ ê°œí¸",
                "ì˜ì–´ ê³µêµìœ¡ íì§€ ë…¼ë€",
                "AI ì‹œëŒ€ ì½”ë”© êµìœ¡ ì˜ë¬´í™”",
                "ì‚¬êµìœ¡ë¹„ ìƒí•œì œ ë„ì…",
                "ëŒ€í•™ ë“±ë¡ê¸ˆ ë¬´ë£Œí™” ì •ì±…"
            ],
            "í™˜ê²½": [
                "í”Œë¼ìŠ¤í‹± ì‚¬ìš© ì „ë©´ ê¸ˆì§€",
                "íƒ„ì†Œì„¸ ë„ì…ì˜ íš¨ê³¼",
                "ì›ì „ vs ì¬ìƒì—ë„ˆì§€ ìš°ì„ ìˆœìœ„",
                "ì¼íšŒìš©í’ˆ ê¸ˆì§€ ì •ì±… í™•ëŒ€",
                "ì „ê¸°ì°¨ ì˜ë¬´í™” ì‹œê¸°"
            ],
            "ê²½ì œ": [
                "4ì¼ ê·¼ë¬´ì œ ë„ì… íš¨ê³¼",
                "ëŒ€ê¸°ì—… ê·œì œ ê°•í™” vs ì™„í™”",
                "ê°€ìƒí™”í ë²•ì •í™”í ì¸ì •",
                "ë¡œë´‡ì„¸ vs ê¸°ìˆ ë°œì „ ììœ ",
                "ë¶€ì˜ ì¬ë¶„ë°° ì •ì±… ê°•í™”"
            ],
            "ê¸ˆìœµ": [
                "ì¼ë°˜ì¸ ì£¼ì‹íˆ¬ìì˜ í•„ìš”ì„±",
                "ê°œì¸íˆ¬ìì ë³´í˜¸ vs ì‹œì¥ ììœ¨ì„±",
                "ì£¼ì‹ ì–‘ë„ì†Œë“ì„¸ ê°•í™” ë…¼ë€",
                "ì²­ì†Œë…„ ê¸ˆìœµêµìœ¡ ì˜ë¬´í™”",
                "ê°€ê³„ë¶€ì±„ í•œë„ ê·œì œ ê°•í™”"
            ]
        }
    
    def show_topic_menu(self) -> str:
        """í† ë¡  ì£¼ì œ ì„ íƒ ë©”ë‰´"""
        print("\nğŸ“š **LangGraph í† ë¡  ì£¼ì œ ì„ íƒ**")
        print("=" * 60)
        
        all_topics = []
        topic_index = 1
        
        for category, topics in self.topic_categories.items():
            print(f"\nğŸ”¸ **{category}** ë¶„ì•¼:")
            for topic in topics:
                print(f"   {topic_index}. {topic}")
                all_topics.append(topic)
                topic_index += 1
        
        print(f"\n   0. ì§ì ‘ ì…ë ¥")
        
        while True:
            try:
                choice = input(f"\nì£¼ì œ ì„ íƒ (0-{len(all_topics)}): ").strip()
                
                if choice == "0":
                    custom_topic = input("í† ë¡  ì£¼ì œë¥¼ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš”: ").strip()
                    if custom_topic:
                        return custom_topic
                    else:
                        print("âš ï¸ ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                        continue
                
                choice_num = int(choice)
                if 1 <= choice_num <= len(all_topics):
                    selected_topic = all_topics[choice_num - 1]
                    print(f"âœ… ì„ íƒëœ ì£¼ì œ: {selected_topic}")
                    return selected_topic
                else:
                    print(f"âš ï¸ 1-{len(all_topics)} ë²”ìœ„ì˜ ìˆ«ìë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                    
            except ValueError:
                print("âš ï¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# ========================
# ì„¤ì • ë° ì‹¤í–‰ í•¨ìˆ˜ë“¤
# ========================

def get_langgraph_config() -> LangGraphConfig:
    """LangGraph ì„¤ì • ìˆ˜ì§‘"""
    print("ğŸ”‘ **LangGraph ì‹œìŠ¤í…œ ì„¤ì •**")
    print("-" * 50)
    
    # OpenAI API í‚¤
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        api_key = input("OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if not api_key:
            print("âŒ API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
            exit(1)
        os.environ["OPENAI_API_KEY"] = api_key
    else:
        print(f"âœ… í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ: {api_key[:8]}...{api_key[-4:]}")
    
    # ëª¨ë¸ ì„ íƒ
    print("\nğŸ§  AI ëª¨ë¸ ì„ íƒ:")
    models = {
        "1": ("gpt-4o-mini", "ë¹ ë¥´ê³  ì €ë ´ (ê¶Œì¥)"),
        "2": ("gpt-4o", "ìµœê³  í’ˆì§ˆ"),
        "3": ("gpt-3.5-turbo", "ê°€ì¥ ì €ë ´")
    }
    
    for key, (model, desc) in models.items():
        print(f"   {key}. {model}: {desc}")
    
    model_choice = input("\nëª¨ë¸ ì„ íƒ (1-3, ê¸°ë³¸ê°’: 1): ").strip()
    selected_model = models.get(model_choice, models["1"])[0]
    
    # ë¼ìš´ë“œ ìˆ˜
    rounds_input = input("í† ë¡  ë¼ìš´ë“œ ìˆ˜ (1-5, ê¸°ë³¸ê°’: 3): ").strip()
    try:
        rounds = int(rounds_input)
        rounds = max(1, min(rounds, 5))
    except:
        rounds = 3
    
    # ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš© ì—¬ë¶€
    checkpoint_input = input("LangGraph ì²´í¬í¬ì¸íŠ¸ ì‚¬ìš©? (Y/n, ê¸°ë³¸ê°’: Y): ").strip().lower()
    enable_checkpoints = checkpoint_input not in ['n', 'no', 'ì•„ë‹ˆìš”']
    
    print(f"\nâœ… **ì„¤ì • ì™„ë£Œ**:")
    print(f"   â€¢ ëª¨ë¸: {selected_model}")
    print(f"   â€¢ ë¼ìš´ë“œ: {rounds}")
    print(f"   â€¢ ì²´í¬í¬ì¸íŠ¸: {'ì‚¬ìš©' if enable_checkpoints else 'ë¯¸ì‚¬ìš©'}")
    
    return LangGraphConfig(
        openai_api_key=api_key,
        model=selected_model,
        max_rounds=rounds,
        enable_checkpoints=enable_checkpoints,
        save_to_file=True
    )

def show_langgraph_features():
    """LangGraph íŠ¹ì§• ì„¤ëª…"""
    print("\n" + "=" * 70)
    print("ğŸŒŸ **LangGraph ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ íŠ¹ì§•**")
    print("=" * 70)
    
    features = {
        "ğŸ§  **ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°**": [
            "StateGraphë¡œ í† ë¡  ìƒíƒœë¥¼ ì¤‘ì•™ ê´€ë¦¬",
            "ê° ë…¸ë“œê°€ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ë©° ì§„í–‰",
            "ë©”ëª¨ë¦¬ ê¸°ë°˜ ëŒ€í™” íˆìŠ¤í† ë¦¬ ìœ ì§€"
        ],
        "ğŸ”„ **ì¡°ê±´ë¶€ ë¼ìš°íŒ…**": [
            "should_continue í•¨ìˆ˜ë¡œ ë™ì  íë¦„ ì œì–´", 
            "í† ë¡  ìƒí™©ì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œ ìë™ ê²°ì •",
            "ë³µì¡í•œ ì¡°ê±´ë¶€ ë¡œì§ êµ¬í˜„ ê°€ëŠ¥"
        ],
        "ğŸ’¾ **ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ**": [
            "MemorySaverë¡œ í† ë¡  ì¤‘ê°„ ìƒíƒœ ì €ì¥",
            "ì¤‘ë‹¨ ì‹œ ì´ì–´ì„œ ì¬ê°œ ê°€ëŠ¥",
            "ë””ë²„ê¹…ê³¼ ë¶„ì„ì— ìœ ìš©"
        ],
        "ğŸ­ **ì§„ì •í•œ ë©€í‹° ì—ì´ì „íŠ¸**": [
            "ê° ì—ì´ì „íŠ¸ê°€ ë…ë¦½ëœ ë…¸ë“œë¡œ ë™ì‘",
            "ë³‘ë ¬ ì²˜ë¦¬ ë° ë¹„ë™ê¸° ì‹¤í–‰ ì§€ì›",
            "í™•ì¥ ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ì•„í‚¤í…ì²˜"
        ],
        "ğŸ“Š **ê³ ê¸‰ ëª¨ë‹ˆí„°ë§**": [
            "ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰ìœ¼ë¡œ ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í™•ì¸",
            "ê° ë‹¨ê³„ë³„ ìƒíƒœ ë³€í™” ì¶”ì ",
            "ì„±ëŠ¥ ë° í† í° ì‚¬ìš©ëŸ‰ ë¶„ì„"
        ]
    }
    
    for category, items in features.items():
        print(f"\n{category}:")
        for item in items:
            print(f"   âœ“ {item}")

def main():
    """LangGraph í† ë¡  ì‹œìŠ¤í…œ ë©”ì¸ í•¨ìˆ˜"""
    try:
        print("ğŸ§  **LangGraph ê¸°ë°˜ ì¸í…”ë¦¬ì „íŠ¸ ë©€í‹° ì—ì´ì „íŠ¸ í† ë¡  ì‹œìŠ¤í…œ**")
        print("   ì§„ì •í•œ ìƒíƒœ ê¸°ë°˜ ê·¸ë˜í”„ ì›Œí¬í”Œë¡œìš°ë¡œ êµ¬í˜„ëœ AI í† ë¡ ")
        print("=" * 70)
        
        # LangGraph íŠ¹ì§• ì„¤ëª…
        show_langgraph_features()
        
        # ì„¤ì • ìˆ˜ì§‘
        config = get_langgraph_config()
        
        # ì£¼ì œ ì„ íƒ
        topic_recommender = LangGraphTopicRecommender()
        topic = topic_recommender.show_topic_menu()
        
        # LangGraph ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        print(f"\nğŸš€ **LangGraph ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...**")
        debate_system = LangGraphDebateSystem(config)
        
        # í† ë¡  ì‹¤í–‰
        final_state = debate_system.run_debate(topic)
        
        if "error" not in final_state:
            total_cost = final_state.get("total_tokens", 0) * 0.00015
            print(f"\nğŸ‰ **LangGraph í† ë¡  ì™„ë£Œ!**")
            print(f"ğŸ’° ì´ ë¹„ìš©: ${total_cost:.4f}")
            print(f"ğŸ”„ ì›Œí¬í”Œë¡œìš°: StateGraph â†’ ì¡°ê±´ë¶€ ë¼ìš°íŒ… â†’ ìë™ íë¦„ ì œì–´")
        
        # ì¶”ê°€ ì˜µì…˜
        print(f"\n" + "=" * 60)
        print(f"ğŸ¯ **ë‹¤ìŒ ë‹¨ê³„ ì„ íƒ**")
        print(f"=" * 60)
        print(f"   1. ìƒˆë¡œìš´ ì£¼ì œë¡œ LangGraph í† ë¡ ")
        print(f"   2. ê°™ì€ ì£¼ì œë¡œ ë‹¤ì‹œ í† ë¡ ")
        print(f"   3. í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
        
        while True:
            choice = input(f"\nì„ íƒí•˜ì„¸ìš” (1-3, ê¸°ë³¸ê°’: 3): ").strip()
            
            if choice == "1":
                main()  # ìƒˆë¡œìš´ í† ë¡ 
                break
            elif choice == "2":
                print(f"\nğŸ”„ ê°™ì€ ì£¼ì œë¡œ LangGraph í† ë¡  ì¬ì‹œì‘...")
                debate_system.run_debate(topic)
                continue
            elif choice == "3" or choice == "":
                print(f"\nğŸ‘‹ LangGraph í† ë¡  ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤!")
                break
            else:
                print(f"âš ï¸ 1-3 ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.")
        
    except KeyboardInterrupt:
        print("\nâ›” ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ LangGraph ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
