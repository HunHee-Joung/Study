#!/usr/bin/env python3
"""
N100 ìµœì í™”ëœ DocLing + Ollama bge-m3 + Qdrant RAG ì‹œìŠ¤í…œ
- íŒŒì¼ëª…ì„ ë©”íƒ€ë°ì´í„°ë¡œ í™œìš©
- Qdrant ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í†µí•©
- ê³ ì„±ëŠ¥ ê²€ìƒ‰ ë° í•„í„°ë§ ì§€ì›
"""

import asyncio
import time
import logging
import psutil
import uuid
import os
import re
import tempfile
import shutil
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from functools import lru_cache
import numpy as np
import requests
from concurrent.futures import ThreadPoolExecutor

# Qdrant ì„í¬íŠ¸
try:
    from qdrant_client import QdrantClient
    from qdrant_client.models import (
        Distance, VectorParams, PointStruct, Filter, 
        FieldCondition, Match, MatchValue, SearchParams
    )
    QDRANT_AVAILABLE = True
except ImportError:
    print("âš ï¸ Qdrantë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install qdrant-client")
    QDRANT_AVAILABLE = False

# DocLing ì„í¬íŠ¸
try:
    from docling.document_converter import DocumentConverter
    from docling.chunking import HybridChunker
    from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer
    from transformers import AutoTokenizer
    DOCLING_AVAILABLE = True
except ImportError:
    print("âš ï¸ DocLingì„ ì„¤ì¹˜í•˜ì„¸ìš”: pip install docling")
    DOCLING_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class QdrantRAGConfig:
    """Qdrant RAG ì‹œìŠ¤í…œ ì„¤ì •"""
    # Qdrant ì„¤ì •
    qdrant_host: str = "localhost"
    qdrant_port: int = 6333
    qdrant_api_key: Optional[str] = None
    collection_name: str = "rag_documents"
    vector_size: int = 1024  # bge-m3 ì„ë² ë”© ì°¨ì›
    
    # Ollama ì„¤ì •
    ollama_base_url: str = "http://localhost:11434"
    embed_model: str = "bge-m3"
    
    # bge-m3 í† í¬ë‚˜ì´ì € ì„¤ì •
    bge_model_id: str = "BAAI/bge-m3"
    max_tokens: int = 512  # N100 ìµœì í™”
    
    # ì²­í‚¹ ì„¤ì •
    chunk_overlap: int = 50
    merge_peers: bool = True
    merge_list_items: bool = True
    
    # ì„±ëŠ¥ ì„¤ì •
    batch_size: int = 3
    max_parallel: int = 2
    timeout: int = 15
    cache_size: int = 1000
    
    # ê²€ìƒ‰ ì„¤ì •
    top_k: int = 10
    similarity_threshold: float = 0.3
    
    # íŒŒì¼ ì²˜ë¦¬ ì„¤ì •
    supported_extensions: List[str] = field(default_factory=lambda: [
        '.pdf', '.docx', '.pptx', '.txt', '.md', '.html'
    ])

@dataclass
class DocumentMetadata:
    """ê°•í™”ëœ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°"""
    file_name: str
    file_path: str
    file_extension: str
    file_size: int
    created_at: float
    processed_at: float
    chunk_index: int
    total_chunks: int
    doc_id: str
    chunk_id: str
    
    # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    source_type: str = "file"  # file, text, url
    language: str = "auto"
    topic: Optional[str] = None
    author: Optional[str] = None
    tags: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (Qdrant í˜¸í™˜)"""
        return {
            'file_name': self.file_name,
            'file_path': self.file_path,
            'file_extension': self.file_extension,
            'file_size': self.file_size,
            'created_at': self.created_at,
            'processed_at': self.processed_at,
            'chunk_index': self.chunk_index,
            'total_chunks': self.total_chunks,
            'doc_id': self.doc_id,
            'chunk_id': self.chunk_id,
            'source_type': self.source_type,
            'language': self.language,
            'topic': self.topic,
            'author': self.author,
            'tags': self.tags
        }

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ í´ë˜ìŠ¤"""
    text: str
    similarity: float
    metadata: DocumentMetadata
    qdrant_id: str

class QdrantManager:
    """Qdrant ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ì"""
    
    def __init__(self, config: QdrantRAGConfig):
        self.config = config
        
        if not QDRANT_AVAILABLE:
            raise ImportError("Qdrant clientê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            self.client = QdrantClient(
                host=config.qdrant_host,
                port=config.qdrant_port,
                api_key=config.qdrant_api_key,
                timeout=config.timeout
            )
            
            # ì»¬ë ‰ì…˜ ì´ˆê¸°í™”
            self._initialize_collection()
            
            logger.info(f"âœ… Qdrant ì—°ê²° ì„±ê³µ: {config.qdrant_host}:{config.qdrant_port}")
            
        except Exception as e:
            logger.error(f"âŒ Qdrant ì—°ê²° ì‹¤íŒ¨: {e}")
            raise
    
    def _initialize_collection(self):
        """ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        try:
            # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
            collections = self.client.get_collections()
            collection_names = [col.name for col in collections.collections]
            
            if self.config.collection_name not in collection_names:
                # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
                self.client.create_collection(
                    collection_name=self.config.collection_name,
                    vectors_config=VectorParams(
                        size=self.config.vector_size,
                        distance=Distance.COSINE
                    )
                )
                logger.info(f"ğŸ“ ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±: {self.config.collection_name}")
            else:
                logger.info(f"ğŸ“ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚¬ìš©: {self.config.collection_name}")
                
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def add_points(self, points: List[PointStruct]) -> bool:
        """í¬ì¸íŠ¸ ë°°ì¹˜ ì¶”ê°€"""
        try:
            self.client.upsert(
                collection_name=self.config.collection_name,
                points=points
            )
            logger.info(f"âœ… {len(points)}ê°œ í¬ì¸íŠ¸ ì¶”ê°€ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"í¬ì¸íŠ¸ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def search_similar(self, 
                      query_vector: List[float], 
                      limit: int = 10,
                      score_threshold: Optional[float] = None,
                      filter_conditions: Optional[Filter] = None) -> List[Dict]:
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        try:
            search_params = SearchParams(
                hnsw_ef=128,  # N100 ìµœì í™”
                exact=False
            )
            
            results = self.client.search(
                collection_name=self.config.collection_name,
                query_vector=query_vector,
                limit=limit,
                score_threshold=score_threshold,
                query_filter=filter_conditions,
                search_params=search_params,
                with_payload=True,
                with_vectors=False
            )
            
            return [
                {
                    'id': result.id,
                    'score': result.score,
                    'payload': result.payload
                }
                for result in results
            ]
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def delete_by_file(self, file_name: str) -> bool:
        """íŒŒì¼ëª…ìœ¼ë¡œ ëª¨ë“  ê´€ë ¨ í¬ì¸íŠ¸ ì‚­ì œ"""
        try:
            filter_condition = Filter(
                must=[
                    FieldCondition(
                        key="file_name",
                        match=MatchValue(value=file_name)
                    )
                ]
            )
            
            self.client.delete(
                collection_name=self.config.collection_name,
                points_selector=filter_condition
            )
            
            logger.info(f"ğŸ—‘ï¸ íŒŒì¼ ê´€ë ¨ ë°ì´í„° ì‚­ì œ: {file_name}")
            return True
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def get_collection_info(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ"""
        try:
            info = self.client.get_collection(self.config.collection_name)
            return {
                'name': info.config.name,
                'vector_size': info.config.params.vectors.size,
                'points_count': info.points_count,
                'segments_count': info.segments_count,
                'status': info.status
            }
        except Exception as e:
            logger.error(f"ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

class FileMetadataExtractor:
    """íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸°"""
    
    @staticmethod
    def extract_file_metadata(file_path: Union[str, Path]) -> Dict[str, Any]:
        """íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        file_path = Path(file_path)
        
        try:
            stat = file_path.stat()
            
            # ê¸°ë³¸ ë©”íƒ€ë°ì´í„°
            metadata = {
                'file_name': file_path.name,
                'file_path': str(file_path.absolute()),
                'file_extension': file_path.suffix.lower(),
                'file_size': stat.st_size,
                'created_at': stat.st_ctime,
                'processed_at': time.time()
            }
            
            # íŒŒì¼ëª…ì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
            file_stem = file_path.stem
            
            # ë‚ ì§œ íŒ¨í„´ ê°ì§€ (YYYY-MM-DD, YYYYMMDD ë“±)
            date_patterns = [
                r'(\d{4})-(\d{2})-(\d{2})',
                r'(\d{4})(\d{2})(\d{2})',
                r'(\d{4})\.(\d{2})\.(\d{2})'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, file_stem)
                if match:
                    metadata['extracted_date'] = f"{match.group(1)}-{match.group(2)}-{match.group(3)}"
                    break
            
            # ë²„ì „ ì •ë³´ ê°ì§€ (v1.0, version_2 ë“±)
            version_pattern = r'v?(\d+)\.?(\d+)?\.?(\d+)?'
            version_match = re.search(version_pattern, file_stem.lower())
            if version_match:
                metadata['version'] = version_match.group(0)
            
            # ì–¸ì–´ ê°ì§€ (íŒŒì¼ëª…ì—ì„œ)
            language_indicators = {
                'ko': ['í•œêµ­ì–´', 'korean', 'í•œê¸€', 'kor'],
                'en': ['english', 'eng', 'en'],
                'zh': ['chinese', 'china', 'zh', 'cn'],
                'ja': ['japanese', 'japan', 'jp']
            }
            
            file_lower = file_stem.lower()
            for lang_code, indicators in language_indicators.items():
                if any(indicator in file_lower for indicator in indicators):
                    metadata['detected_language'] = lang_code
                    break
            
            # ë¬¸ì„œ íƒ€ì… ì¶”ì •
            doc_type_keywords = {
                'manual': ['manual', 'ë§¤ë‰´ì–¼', 'ì„¤ëª…ì„œ', 'guide'],
                'report': ['report', 'ë³´ê³ ì„œ', 'analysis', 'ë¶„ì„'],
                'presentation': ['presentation', 'ë°œí‘œ', 'ppt'],
                'specification': ['spec', 'ëª…ì„¸ì„œ', 'specification'],
                'tutorial': ['tutorial', 'íŠœí† ë¦¬ì–¼', 'howto', 'ë°©ë²•']
            }
            
            for doc_type, keywords in doc_type_keywords.items():
                if any(keyword in file_lower for keyword in keywords):
                    metadata['document_type'] = doc_type
                    break
            
            return metadata
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ {file_path}: {e}")
            return {
                'file_name': file_path.name,
                'file_path': str(file_path),
                'file_extension': file_path.suffix.lower(),
                'file_size': 0,
                'created_at': time.time(),
                'processed_at': time.time()
            }

class OllamaEmbeddingClient:
    """Ollama ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ (ìºì‹± ê°•í™”)"""
    
    def __init__(self, config: QdrantRAGConfig):
        self.config = config
        self.session = requests.Session()
        
        # ì—°ê²° í’€ ìµœì í™”
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=config.max_parallel,
            pool_maxsize=config.max_parallel * 2,
            max_retries=3
        )
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        
        # ìºì‹œ (ë©”ëª¨ë¦¬ + ê°„ë‹¨í•œ íŒŒì¼ ìºì‹œ)
        self._memory_cache = {}
        self._cache_stats = {'hits': 0, 'misses': 0}
        
    @lru_cache(maxsize=1000)
    def _cache_key(self, text: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return f"bge_m3_{hash(text[:200])}"  # ì²« 200ì í•´ì‹œ
    
    def embed_text(self, text: str) -> Optional[np.ndarray]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© (ìºì‹± í¬í•¨)"""
        cache_key = self._cache_key(text)
        
        # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
        if cache_key in self._memory_cache:
            self._cache_stats['hits'] += 1
            return self._memory_cache[cache_key]
        
        self._cache_stats['misses'] += 1
        
        try:
            # í† í° ìˆ˜ ì œí•œ
            limited_text = text[:self.config.max_tokens * 4]
            
            response = self.session.post(
                f"{self.config.ollama_base_url}/api/embeddings",
                json={
                    "model": self.config.embed_model,
                    "prompt": limited_text
                },
                timeout=self.config.timeout
            )
            
            if response.status_code == 200:
                data = response.json()
                embedding = np.array(data["embedding"], dtype=np.float32)
                
                # ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥ (í¬ê¸° ì œí•œ)
                if len(self._memory_cache) < self.config.cache_size:
                    self._memory_cache[cache_key] = embedding
                
                return embedding
            else:
                logger.error(f"Ollama ì„ë² ë”© ì‹¤íŒ¨: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìš”ì²­ ì˜¤ë¥˜: {e}")
            return None
    
    def embed_batch(self, texts: List[str]) -> List[Optional[np.ndarray]]:
        """ë°°ì¹˜ ì„ë² ë”©"""
        with ThreadPoolExecutor(max_workers=self.config.max_parallel) as executor:
            futures = [executor.submit(self.embed_text, text) for text in texts]
            results = []
            
            for future in futures:
                try:
                    result = future.result(timeout=self.config.timeout)
                    results.append(result)
                except Exception as e:
                    logger.error(f"ë°°ì¹˜ ì„ë² ë”© ì˜¤ë¥˜: {e}")
                    results.append(None)
            
            return results
    
    def check_ollama_status(self) -> bool:
        """Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = self.session.get(
                f"{self.config.ollama_base_url}/api/tags", 
                timeout=5
            )
            
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [model['name'] for model in models]
                
                if self.config.embed_model in model_names:
                    logger.info(f"âœ… {self.config.embed_model} ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
                    return True
                else:
                    logger.warning(f"âŒ {self.config.embed_model} ëª¨ë¸ ì—†ìŒ")
                    logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {model_names}")
                    logger.info(f"ì„¤ì¹˜ ëª…ë ¹: ollama pull {self.config.embed_model}")
                    return False
            else:
                logger.error(f"Ollama API ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
            logger.info("Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: ollama serve")
            return False
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„"""
        total_requests = self._cache_stats['hits'] + self._cache_stats['misses']
        hit_rate = self._cache_stats['hits'] / total_requests if total_requests > 0 else 0
        
        return {
            'cache_hits': self._cache_stats['hits'],
            'cache_misses': self._cache_stats['misses'],
            'hit_rate': hit_rate,
            'cache_size': len(self._memory_cache)
        }

class DoclingProcessor:
    """DocLing ë¬¸ì„œ ì²˜ë¦¬ê¸° (íŒŒì¼ ë©”íƒ€ë°ì´í„° í†µí•©)"""
    
    def __init__(self, config: QdrantRAGConfig):
        self.config = config
        
        if not DOCLING_AVAILABLE:
            raise ImportError("DocLingì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        try:
            tokenizer = HuggingFaceTokenizer(
                tokenizer=AutoTokenizer.from_pretrained(config.bge_model_id),
                max_tokens=config.max_tokens
            )
            
            # HybridChunker ì´ˆê¸°í™”
            self.chunker = HybridChunker(
                tokenizer=tokenizer,
                merge_peers=config.merge_peers,
                merge_list_items=config.merge_list_items
            )
            
            # ë¬¸ì„œ ë³€í™˜ê¸°
            self.converter = DocumentConverter()
            
            logger.info("âœ… DocLing í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"DocLing ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def process_file(self, file_path: Union[str, Path]) -> Tuple[List[str], Dict[str, Any]]:
        """íŒŒì¼ ì²˜ë¦¬ ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        file_path = Path(file_path)
        
        try:
            # íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            file_metadata = FileMetadataExtractor.extract_file_metadata(file_path)
            
            # í™•ì¥ì í™•ì¸
            if file_metadata['file_extension'] not in self.config.supported_extensions:
                logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_metadata['file_extension']}")
                return [], file_metadata
            
            logger.info(f"ğŸ“„ íŒŒì¼ ì²˜ë¦¬ ì¤‘: {file_metadata['file_name']}")
            
            # ë¬¸ì„œ ë³€í™˜
            converted_doc = self.converter.convert(source=str(file_path))
            
            # ì²­í‚¹ ë° ì»¨í…ìŠ¤íŠ¸í™”
            chunks = []
            for chunk in self.chunker.chunk(dl_doc=converted_doc.document):
                # contextualize ì ìš© (serialize ë©”ì†Œë“œ ì‚¬ìš©)
                contextualized_text = self.chunker.serialize(chunk)
                chunks.append(contextualized_text)
            
            logger.info(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            return chunks, file_metadata
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
            return [], FileMetadataExtractor.extract_file_metadata(file_path)

class QdrantRAGSystem:
    """Qdrant ê¸°ë°˜ ì™„ì „í•œ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: QdrantRAGConfig):
        self.config = config
        
        # êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self.qdrant_manager = QdrantManager(config)
        self.embedding_client = OllamaEmbeddingClient(config)
        
        if DOCLING_AVAILABLE:
            self.docling_processor = DoclingProcessor(config)
        else:
            self.docling_processor = None
            logger.warning("DocLing ë¹„í™œì„±í™” - í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë“œ")
        
        # ì„±ëŠ¥ í†µê³„
        self.performance_stats = {
            'files_processed': 0,
            'chunks_created': 0,
            'searches_performed': 0,
            'avg_search_time': 0.0,
            'total_processing_time': 0.0
        }
    
    def add_file(self, file_path: Union[str, Path], 
                 additional_metadata: Optional[Dict] = None) -> bool:
        """íŒŒì¼ ì¶”ê°€ (ì „ì²´ íŒŒì´í”„ë¼ì¸)"""
        start_time = time.time()
        file_path = Path(file_path)
        
        try:
            if not file_path.exists():
                logger.error(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
                return False
            
            # DocLingìœ¼ë¡œ íŒŒì¼ ì²˜ë¦¬
            if self.docling_processor:
                chunks, file_metadata = self.docling_processor.process_file(file_path)
            else:
                # í…ìŠ¤íŠ¸ íŒŒì¼ ì§ì ‘ ì²˜ë¦¬
                chunks, file_metadata = self._process_text_file(file_path)
            
            if not chunks:
                logger.warning(f"ì²˜ë¦¬í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                return False
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„° ë³‘í•©
            if additional_metadata:
                file_metadata.update(additional_metadata)
            
            # ë¬¸ì„œ ID ìƒì„±
            doc_id = str(uuid.uuid4())
            
            # ì²­í¬ ì„ë² ë”© ë° Qdrant ì €ì¥
            success = self._embed_and_store_chunks(chunks, file_metadata, doc_id)
            
            if success:
                # í†µê³„ ì—…ë°ì´íŠ¸
                processing_time = time.time() - start_time
                self.performance_stats['files_processed'] += 1
                self.performance_stats['chunks_created'] += len(chunks)
                self.performance_stats['total_processing_time'] += processing_time
                
                logger.info(f"âœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {file_metadata['file_name']} ({processing_time:.2f}ì´ˆ)")
                return True
            else:
                logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file_metadata['file_name']}")
                return False
                
        except Exception as e:
            logger.error(f"íŒŒì¼ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _process_text_file(self, file_path: Path) -> Tuple[List[str], Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ íŒŒì¼ ì§ì ‘ ì²˜ë¦¬"""
        try:
            # íŒŒì¼ ë©”íƒ€ë°ì´í„°
            file_metadata = FileMetadataExtractor.extract_file_metadata(file_path)
            
            # í…ìŠ¤íŠ¸ ì½ê¸°
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ê°„ë‹¨í•œ ì²­í‚¹
            chunks = self._simple_chunk_text(content)
            
            return chunks, file_metadata
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return [], FileMetadataExtractor.extract_file_metadata(file_path)
    
    def _simple_chunk_text(self, text: str) -> List[str]:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì²­í‚¹"""
        max_chars = self.config.max_tokens * 3
        
        if len(text) <= max_chars:
            return [text]
        
        chunks = []
        paragraphs = text.split('\n\n')
        
        current_chunk = ""
        for paragraph in paragraphs:
            if len(current_chunk) + len(paragraph) <= max_chars:
                current_chunk += paragraph + "\n\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = paragraph + "\n\n"
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _embed_and_store_chunks(self, chunks: List[str], 
                               file_metadata: Dict[str, Any], 
                               doc_id: str) -> bool:
        """ì²­í¬ ì„ë² ë”© ë° Qdrant ì €ì¥"""
        try:
            logger.info(f"ğŸ“ {len(chunks)}ê°œ ì²­í¬ ì„ë² ë”© ì¤‘...")
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”©
            all_embeddings = []
            for i in range(0, len(chunks), self.config.batch_size):
                batch = chunks[i:i + self.config.batch_size]
                batch_embeddings = self.embedding_client.embed_batch(batch)
                all_embeddings.extend(batch_embeddings)
                
                # N100 ì—´ ê´€ë¦¬
                time.sleep(0.2)
                
                progress = min(i + self.config.batch_size, len(chunks))
                logger.info(f"   ì„ë² ë”© ì§„í–‰ë¥ : {progress}/{len(chunks)}")
            
            # Qdrant í¬ì¸íŠ¸ ìƒì„±
            points = []
            successful_chunks = 0
            
            for i, (chunk, embedding) in enumerate(zip(chunks, all_embeddings)):
                if embedding is not None:
                    chunk_id = f"{doc_id}_chunk_{i}"
                    
                    # ë©”íƒ€ë°ì´í„° ê°ì²´ ìƒì„±
                    metadata = DocumentMetadata(
                        file_name=file_metadata['file_name'],
                        file_path=file_metadata['file_path'],
                        file_extension=file_metadata['file_extension'],
                        file_size=file_metadata['file_size'],
                        created_at=file_metadata['created_at'],
                        processed_at=file_metadata['processed_at'],
                        chunk_index=i,
                        total_chunks=len(chunks),
                        doc_id=doc_id,
                        chunk_id=chunk_id,
                        
                        # ì¶”ê°€ ì •ë³´
                        language=file_metadata.get('detected_language', 'auto'),
                        topic=file_metadata.get('document_type'),
                        tags=file_metadata.get('tags', [])
                    )
                    
                    # Qdrant í¬ì¸íŠ¸ ìƒì„±
                    point = PointStruct(
                        id=chunk_id,
                        vector=embedding.tolist(),
                        payload={
                            **metadata.to_dict(),
                            'text': chunk,
                            'text_length': len(chunk)
                        }
                    )
                    
                    points.append(point)
                    successful_chunks += 1
            
            # Qdrantì— ë°°ì¹˜ ì €ì¥
            if points:
                success = self.qdrant_manager.add_points(points)
                if success:
                    logger.info(f"âœ… {successful_chunks}/{len(chunks)} ì²­í¬ Qdrant ì €ì¥ ì™„ë£Œ")
                    return True
                else:
                    logger.error("Qdrant ì €ì¥ ì‹¤íŒ¨")
                    return False
            else:
                logger.warning("ì €ì¥í•  ìœ íš¨í•œ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
                
        except Exception as e:
            logger.error(f"ì„ë² ë”© ë° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def search(self, query: str, 
               top_k: Optional[int] = None,
               file_filter: Optional[str] = None,
               extension_filter: Optional[str] = None,
               date_range: Optional[Tuple[float, float]] = None) -> List[SearchResult]:
        """ê³ ê¸‰ ê²€ìƒ‰ (í•„í„°ë§ ì§€ì›)"""
        start_time = time.time()
        top_k = top_k or self.config.top_k
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.embedding_client.embed_text(query)
            if query_embedding is None:
                logger.error("ì¿¼ë¦¬ ì„ë² ë”© ì‹¤íŒ¨")
                return []
            
            # í•„í„° ì¡°ê±´ êµ¬ì„±
            filter_conditions = []
            
            if file_filter:
                filter_conditions.append(
                    FieldCondition(
                        key="file_name",
                        match=MatchValue(value=file_filter)
                    )
                )
            
            if extension_filter:
                filter_conditions.append(
                    FieldCondition(
                        key="file_extension",
                        match=MatchValue(value=extension_filter)
                    )
                )
            
            if date_range:
                start_date, end_date = date_range
                filter_conditions.append(
                    FieldCondition(
                        key="created_at",
                        range={
                            "gte": start_date,
                            "lte": end_date
                        }
                    )
                )
            
            # í•„í„° ê°ì²´ ìƒì„±
            query_filter = None
            if filter_conditions:
                query_filter = Filter(must=filter_conditions)
            
            # Qdrant ê²€ìƒ‰
            search_results = self.qdrant_manager.search_similar(
                query_vector=query_embedding.tolist(),
                limit=top_k,
                score_threshold=self.config.similarity_threshold,
                filter_conditions=query_filter
            )
            
            # SearchResult ê°ì²´ë¡œ ë³€í™˜
            results = []
            for result in search_results:
                payload = result['payload']
                
                metadata = DocumentMetadata(
                    file_name=payload['file_name'],
                    file_path=payload['file_path'],
                    file_extension=payload['file_extension'],
                    file_size=payload['file_size'],
                    created_at=payload['created_at'],
                    processed_at=payload['processed_at'],
                    chunk_index=payload['chunk_index'],
                    total_chunks=payload['total_chunks'],
                    doc_id=payload['doc_id'],
                    chunk_id=payload['chunk_id'],
                    source_type=payload.get('source_type', 'file'),
                    language=payload.get('language', 'auto'),
                    topic=payload.get('topic'),
                    author=payload.get('author'),
                    tags=payload.get('tags', [])
                )
                
                search_result = SearchResult(
                    text=payload['text'],
                    similarity=result['score'],
                    metadata=metadata,
                    qdrant_id=result['id']
                )
                
                results.append(search_result)
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            search_time = time.time() - start_time
            self.performance_stats['searches_performed'] += 1
            
            current_avg = self.performance_stats['avg_search_time']
            searches_count = self.performance_stats['searches_performed']
            self.performance_stats['avg_search_time'] = (
                (current_avg * (searches_count - 1) + search_time) / searches_count
            )
            
            logger.info(f"ğŸ” ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼, {search_time:.3f}ì´ˆ")
            return results
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_by_filename(self, filename: str, top_k: int = 5) -> List[SearchResult]:
        """íŒŒì¼ëª…ìœ¼ë¡œ ê²€ìƒ‰"""
        return self.search("", top_k=top_k, file_filter=filename)
    
    def search_by_extension(self, extension: str, query: str = "", top_k: int = 10) -> List[SearchResult]:
        """íŒŒì¼ í™•ì¥ìë¡œ ê²€ìƒ‰"""
        if not extension.startswith('.'):
            extension = '.' + extension
        return self.search(query, top_k=top_k, extension_filter=extension)
    
    def get_file_list(self) -> List[Dict[str, Any]]:
        """ì €ì¥ëœ íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
        try:
            # ëª¨ë“  ê³ ìœ  íŒŒì¼ëª… ì¡°íšŒ (Qdrant scroll ì‚¬ìš©)
            scroll_result = self.qdrant_manager.client.scroll(
                collection_name=self.config.collection_name,
                limit=1000,  # ì ì ˆí•œ í¬ê¸°ë¡œ ì¡°ì •
                with_payload=True,
                with_vectors=False
            )
            
            files = {}
            for point in scroll_result[0]:
                payload = point.payload
                file_name = payload['file_name']
                
                if file_name not in files:
                    files[file_name] = {
                        'file_name': file_name,
                        'file_path': payload['file_path'],
                        'file_extension': payload['file_extension'],
                        'file_size': payload['file_size'],
                        'created_at': payload['created_at'],
                        'processed_at': payload['processed_at'],
                        'total_chunks': payload['total_chunks'],
                        'language': payload.get('language', 'auto'),
                        'topic': payload.get('topic'),
                        'tags': payload.get('tags', [])
                    }
            
            return list(files.values())
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def delete_file(self, filename: str) -> bool:
        """íŒŒì¼ ë° ê´€ë ¨ ì²­í¬ ì‚­ì œ"""
        try:
            success = self.qdrant_manager.delete_by_file(filename)
            if success:
                logger.info(f"ğŸ—‘ï¸ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {filename}")
            return success
        except Exception as e:
            logger.error(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´"""
        memory = psutil.virtual_memory()
        cpu_percent = psutil.cpu_percent(interval=1)
        
        # Qdrant ì •ë³´
        qdrant_info = self.qdrant_manager.get_collection_info()
        
        # ìºì‹œ í†µê³„
        cache_stats = self.embedding_client.get_cache_stats()
        
        return {
            'performance': {
                'files_processed': self.performance_stats['files_processed'],
                'chunks_created': self.performance_stats['chunks_created'],
                'searches_performed': self.performance_stats['searches_performed'],
                'avg_search_time': self.performance_stats['avg_search_time'],
                'total_processing_time': self.performance_stats['total_processing_time']
            },
            'qdrant': qdrant_info,
            'cache': cache_stats,
            'system': {
                'memory_usage_percent': memory.percent,
                'cpu_usage_percent': cpu_percent,
                'memory_available_gb': memory.available / (1024**3)
            }
        }

class QdrantRAGTester:
    """Qdrant RAG ì‹œìŠ¤í…œ í…ŒìŠ¤í„°"""
    
    def __init__(self, rag_system: QdrantRAGSystem):
        self.rag = rag_system
    
    def run_comprehensive_test(self):
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ§ª Qdrant RAG ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # 1. Qdrant ì—°ê²° í…ŒìŠ¤íŠ¸
        self._test_qdrant_connection()
        
        # 2. Ollama ì—°ê²° í…ŒìŠ¤íŠ¸
        self._test_ollama_connection()
        
        # 3. íŒŒì¼ ì¶”ê°€ í…ŒìŠ¤íŠ¸
        self._test_file_processing()
        
        # 4. ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
        self._test_search_functionality()
        
        # 5. í•„í„°ë§ í…ŒìŠ¤íŠ¸
        self._test_filtering()
        
        # 6. íŒŒì¼ ê´€ë¦¬ í…ŒìŠ¤íŠ¸
        self._test_file_management()
        
        # 7. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        self._test_performance()
        
        # 8. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        self._check_system_status()
    
    def _test_qdrant_connection(self):
        """Qdrant ì—°ê²° í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”Œ Qdrant ì—°ê²° í…ŒìŠ¤íŠ¸...")
        
        try:
            collection_info = self.rag.qdrant_manager.get_collection_info()
            if collection_info:
                logger.info(f"âœ… Qdrant ì—°ê²° ì„±ê³µ: {collection_info['name']}")
                logger.info(f"   í¬ì¸íŠ¸ ìˆ˜: {collection_info['points_count']}")
                logger.info(f"   ë²¡í„° ì°¨ì›: {collection_info['vector_size']}")
                return True
            else:
                logger.error("âŒ Qdrant ì—°ê²° ì‹¤íŒ¨")
                return False
        except Exception as e:
            logger.error(f"âŒ Qdrant í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
            return False
    
    def _test_ollama_connection(self):
        """Ollama ì—°ê²° í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ¤– Ollama ì—°ê²° í…ŒìŠ¤íŠ¸...")
        
        if self.rag.embedding_client.check_ollama_status():
            # ê°„ë‹¨í•œ ì„ë² ë”© í…ŒìŠ¤íŠ¸
            test_embedding = self.rag.embedding_client.embed_text("í…ŒìŠ¤íŠ¸ ë¬¸ì¥")
            if test_embedding is not None:
                logger.info(f"âœ… ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì„±ê³µ (ì°¨ì›: {len(test_embedding)})")
                return True
            else:
                logger.error("âŒ ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                return False
        else:
            logger.error("âŒ Ollama ì—°ê²° ì‹¤íŒ¨")
            return False
    
    def _test_file_processing(self):
        """íŒŒì¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ“ íŒŒì¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        
        # í…ŒìŠ¤íŠ¸ìš© ì„ì‹œ íŒŒì¼ ìƒì„±
        test_files = self._create_test_files()
        
        success_count = 0
        for file_path, content in test_files.items():
            logger.info(f"ğŸ“„ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬: {Path(file_path).name}")
            
            if self.rag.add_file(file_path, {'test': True, 'tags': ['test_file']}):
                success_count += 1
                logger.info(f"âœ… íŒŒì¼ ì²˜ë¦¬ ì„±ê³µ: {Path(file_path).name}")
            else:
                logger.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {Path(file_path).name}")
        
        logger.info(f"ğŸ“Š íŒŒì¼ ì²˜ë¦¬ ê²°ê³¼: {success_count}/{len(test_files)}")
        
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬
        self._cleanup_test_files(test_files.keys())
        
        return success_count == len(test_files)
    
    def _create_test_files(self) -> Dict[str, str]:
        """í…ŒìŠ¤íŠ¸ìš© íŒŒì¼ ìƒì„±"""
        test_files = {}
        temp_dir = tempfile.mkdtemp()
        
        # í•œêµ­ì–´ ë¬¸ì„œ
        korean_content = """
# IBM ì¸ê³µì§€ëŠ¥ ê¸°ìˆ  ë³´ê³ ì„œ 2024

## ê°œìš”
IBMì€ ì¸ê³µì§€ëŠ¥ ë¶„ì•¼ì—ì„œ í˜ì‹ ì ì¸ ê¸°ìˆ ì„ ì§€ì†ì ìœ¼ë¡œ ê°œë°œí•˜ê³  ìˆìŠµë‹ˆë‹¤.
Watson AI í”Œë«í¼ì„ í†µí•´ ê¸°ì—…ë“¤ì´ ë°ì´í„°ë¥¼ í™œìš©í•œ ì˜ì‚¬ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤.

## ì£¼ìš” ê¸°ìˆ 
- ìì—°ì–´ ì²˜ë¦¬ (NLP)
- ê¸°ê³„í•™ìŠµ (Machine Learning)
- ë”¥ëŸ¬ë‹ (Deep Learning)
- ì»´í“¨í„° ë¹„ì „ (Computer Vision)

## DocLing ê¸°ìˆ 
DocLingì€ IBM Researchì—ì„œ ê°œë°œí•œ ë¬¸ì„œ ë³€í™˜ ë„êµ¬ì…ë‹ˆë‹¤.
HybridChunkerì˜ contextualize ê¸°ëŠ¥ìœ¼ë¡œ ë¬¸ì„œì˜ êµ¬ì¡°ì  ì •ë³´ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤.
"""
        
        # ì˜ì–´ ë¬¸ì„œ
        english_content = """
# IBM AI Technology Report 2024

## Overview
IBM continues to develop innovative AI technologies.
The Watson AI platform helps enterprises make data-driven decisions.

## Key Technologies
- Natural Language Processing (NLP)
- Machine Learning
- Deep Learning
- Computer Vision

## DocLing Technology
DocLing is a document conversion tool developed by IBM Research.
The contextualize feature of HybridChunker preserves structural information.
"""
        
        # ê¸°ìˆ  ë¬¸ì„œ
        tech_spec = """
# bge-m3 ì„ë² ë”© ëª¨ë¸ ëª…ì„¸ì„œ

## ëª¨ë¸ ì •ë³´
- ê°œë°œì‚¬: BAAI (Beijing Academy of Artificial Intelligence)
- ëª¨ë¸ í¬ê¸°: 2.5GB
- ì„ë² ë”© ì°¨ì›: 1024
- ìµœëŒ€ í† í°: 8192

## ì§€ì› ì–¸ì–´
í•œêµ­ì–´, ì˜ì–´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´ ë“± 100+ ì–¸ì–´ ì§€ì›

## ì„±ëŠ¥
MTEB ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì„
íŠ¹íˆ ë‹¤êµ­ì–´ ê²€ìƒ‰ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ê²°ê³¼
"""
        
        files_to_create = {
            'ibm_ai_report_korean_2024.md': korean_content,
            'ibm_ai_report_english_2024.md': english_content,
            'bge_m3_specification_v1.0.md': tech_spec
        }
        
        for filename, content in files_to_create.items():
            file_path = os.path.join(temp_dir, filename)
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            test_files[file_path] = content
        
        return test_files
    
    def _cleanup_test_files(self, file_paths):
        """í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬"""
        temp_dirs = set()
        for file_path in file_paths:
            temp_dirs.add(os.path.dirname(file_path))
        
        for temp_dir in temp_dirs:
            try:
                shutil.rmtree(temp_dir)
                logger.info(f"ğŸ§¹ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬: {temp_dir}")
            except Exception as e:
                logger.warning(f"í…ŒìŠ¤íŠ¸ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _test_search_functionality(self):
        """ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ” ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        test_queries = [
            "IBM ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "DocLingì˜ HybridChunker ê¸°ëŠ¥ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "bge-m3 ëª¨ë¸ì˜ ì„±ëŠ¥ê³¼ íŠ¹ì§•ì€?",
            "ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ",
            "Watson AI platform capabilities"
        ]
        
        for i, query in enumerate(test_queries, 1):
            logger.info(f"\nğŸ” ì¿¼ë¦¬ {i}: {query}")
            
            results = self.rag.search(query, top_k=3)
            
            if results:
                logger.info(f"ğŸ“„ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬:")
                for j, result in enumerate(results, 1):
                    logger.info(f"  {j}. ìœ ì‚¬ë„: {result.similarity:.3f}")
                    logger.info(f"     íŒŒì¼: {result.metadata.file_name}")
                    logger.info(f"     ë‚´ìš©: {result.text[:100]}...")
            else:
                logger.warning("âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
    
    def _test_filtering(self):
        """í•„í„°ë§ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ¯ í•„í„°ë§ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        # íŒŒì¼ëª… í•„í„°
        logger.info("\nğŸ“ íŒŒì¼ëª… í•„í„° í…ŒìŠ¤íŠ¸:")
        results = self.rag.search_by_filename("ibm_ai_report_korean_2024.md")
        logger.info(f"í•œêµ­ì–´ ë³´ê³ ì„œ ê²°ê³¼: {len(results)}ê°œ")
        
        # í™•ì¥ì í•„í„°
        logger.info("\nğŸ“ í™•ì¥ì í•„í„° í…ŒìŠ¤íŠ¸:")
        results = self.rag.search_by_extension(".md", "IBM")
        logger.info(f"ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
        
        # ë³µí•© í•„í„°
        logger.info("\nğŸ”— ë³µí•© í•„í„° í…ŒìŠ¤íŠ¸:")
        current_time = time.time()
        one_hour_ago = current_time - 3600
        results = self.rag.search(
            "ê¸°ìˆ ", 
            top_k=5,
            extension_filter=".md",
            date_range=(one_hour_ago, current_time)
        )
        logger.info(f"ìµœê·¼ 1ì‹œê°„ ë‚´ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²€ìƒ‰: {len(results)}ê°œ")
    
    def _test_file_management(self):
        """íŒŒì¼ ê´€ë¦¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ“‚ íŒŒì¼ ê´€ë¦¬ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        # íŒŒì¼ ëª©ë¡ ì¡°íšŒ
        file_list = self.rag.get_file_list()
        logger.info(f"ğŸ“‹ ì €ì¥ëœ íŒŒì¼ ìˆ˜: {len(file_list)}")
        
        for file_info in file_list[:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
            logger.info(f"  ğŸ“„ {file_info['file_name']}")
            logger.info(f"     í¬ê¸°: {file_info['file_size']} bytes")
            logger.info(f"     ì²­í¬ ìˆ˜: {file_info['total_chunks']}")
            logger.info(f"     ì–¸ì–´: {file_info.get('language', 'auto')}")
    
    def _test_performance(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("âš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        # ë°˜ë³µ ê²€ìƒ‰ìœ¼ë¡œ ìºì‹œ íš¨ê³¼ ì¸¡ì •
        test_query = "IBM Watson"
        times = []
        
        logger.info(f"ğŸ”„ ë°˜ë³µ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ì¿¼ë¦¬: '{test_query}')")
        for i in range(5):
            start_time = time.time()
            results = self.rag.search(test_query, top_k=5)
            search_time = time.time() - start_time
            times.append(search_time)
            logger.info(f"  ê²€ìƒ‰ {i+1}: {search_time:.3f}ì´ˆ ({len(results)}ê°œ ê²°ê³¼)")
        
        avg_time = sum(times) / len(times)
        min_time = min(times)
        max_time = max(times)
        
        logger.info(f"ğŸ“Š ê²€ìƒ‰ ì„±ëŠ¥ í†µê³„:")
        logger.info(f"  í‰ê· : {avg_time:.3f}ì´ˆ")
        logger.info(f"  ìµœì†Œ: {min_time:.3f}ì´ˆ")
        logger.info(f"  ìµœëŒ€: {max_time:.3f}ì´ˆ")
        
        # ìºì‹œ íš¨ê³¼ í™•ì¸
        cache_stats = self.rag.embedding_client.get_cache_stats()
        logger.info(f"ğŸ’¾ ìºì‹œ í†µê³„:")
        logger.info(f"  íˆíŠ¸ìœ¨: {cache_stats['hit_rate']:.1%}")
        logger.info(f"  ìºì‹œ í¬ê¸°: {cache_stats['cache_size']}")
    
    def _check_system_status(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        logger.info("ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸...")
        
        status = self.rag.get_system_status()
        
        logger.info(f"ğŸ¯ ì„±ëŠ¥ í†µê³„:")
        perf = status['performance']
        logger.info(f"  ì²˜ë¦¬ëœ íŒŒì¼: {perf['files_processed']}")
        logger.info(f"  ìƒì„±ëœ ì²­í¬: {perf['chunks_created']}")
        logger.info(f"  ìˆ˜í–‰ëœ ê²€ìƒ‰: {perf['searches_performed']}")
        logger.info(f"  í‰ê·  ê²€ìƒ‰ ì‹œê°„: {perf['avg_search_time']:.3f}ì´ˆ")
        
        logger.info(f"ğŸ—ƒï¸ Qdrant ìƒíƒœ:")
        qdrant = status['qdrant']
        logger.info(f"  ì»¬ë ‰ì…˜: {qdrant.get('name', 'N/A')}")
        logger.info(f"  í¬ì¸íŠ¸ ìˆ˜: {qdrant.get('points_count', 0)}")
        logger.info(f"  ìƒíƒœ: {qdrant.get('status', 'N/A')}")
        
        logger.info(f"ğŸ’» ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤:")
        system = status['system']
        logger.info(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {system['memory_usage_percent']:.1f}%")
        logger.info(f"  CPU ì‚¬ìš©ë¥ : {system['cpu_usage_percent']:.1f}%")
        logger.info(f"  ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {system['memory_available_gb']:.1f}GB")

def setup_and_run_qdrant_demo():
    """Qdrant RAG ë°ëª¨ ì„¤ì • ë° ì‹¤í–‰"""
    print("ğŸš€ N100 + DocLing + Ollama bge-m3 + Qdrant RAG ì‹œìŠ¤í…œ ë°ëª¨")
    print("=" * 70)
    
    # ì„¤ì •
    config = QdrantRAGConfig(
        # Qdrant ì„¤ì •
        qdrant_host="localhost",
        qdrant_port=6333,
        collection_name="n100_rag_docs",
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        max_tokens=512,
        batch_size=3,
        max_parallel=2,
        cache_size=500,
        top_k=10
    )
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    try:
        rag_system = QdrantRAGSystem(config)
        logger.info("âœ… Qdrant RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("\nğŸ’¡ ì„¤ì¹˜ê°€ í•„ìš”í•œ êµ¬ì„± ìš”ì†Œ:")
        print("1. Qdrant: docker run -p 6333:6333 qdrant/qdrant")
        print("2. Ollama: curl -fsSL https://ollama.ai/install.sh | sh")
        print("3. bge-m3: ollama pull bge-m3")
        print("4. Python íŒ¨í‚¤ì§€: pip install qdrant-client docling")
        return
    
    # í…ŒìŠ¤í„° ì‹¤í–‰
    tester = QdrantRAGTester(rag_system)
    tester.run_comprehensive_test()
    
    print("\n" + "=" * 70)
    print("ğŸ¯ Qdrant + íŒŒì¼ëª… ë©”íƒ€ë°ì´í„° ì‹œìŠ¤í…œ íŠ¹ì§•:")
    print("â€¢ íŒŒì¼ëª… ìë™ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ë‚ ì§œ, ë²„ì „, ì–¸ì–´, ë¬¸ì„œíƒ€ì…)")
    print("â€¢ Qdrant ë²¡í„° DBë¡œ ë¹ ë¥¸ ìœ ì‚¬ë„ ê²€ìƒ‰")
    print("â€¢ íŒŒì¼ëª…/í™•ì¥ì/ë‚ ì§œ ë²”ìœ„ë³„ í•„í„°ë§")
    print("â€¢ N100 CPU ìµœì í™” (ë°°ì¹˜ ì²˜ë¦¬, ìºì‹±, ì—´ê´€ë¦¬)")
    print("â€¢ DocLing contextualizeë¡œ ë¬¸ë§¥ ë³´ì¡´")
    
    print("\nğŸ’¡ ì£¼ìš” ì‚¬ìš©ë²•:")
    print("â€¢ íŒŒì¼ ì¶”ê°€: rag.add_file('/path/to/document.pdf')")
    print("â€¢ ê²€ìƒ‰: rag.search('ì¿¼ë¦¬', top_k=5)")
    print("â€¢ íŒŒì¼ë³„ ê²€ìƒ‰: rag.search_by_filename('report.pdf')")
    print("â€¢ í™•ì¥ìë³„ ê²€ìƒ‰: rag.search_by_extension('.md', 'ë‚´ìš©')")
    print("â€¢ íŒŒì¼ ì‚­ì œ: rag.delete_file('filename.pdf')")
    
    print("\nğŸ”§ ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ:")
    print("""
# RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
config = QdrantRAGConfig()
rag = QdrantRAGSystem(config)

# íŒŒì¼ ì¶”ê°€
rag.add_file('reports/IBM_AI_Report_2024.pdf')
rag.add_file('specs/bge-m3_manual_korean.docx')

# ê²€ìƒ‰
results = rag.search('IBM ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ', top_k=5)
for result in results:
    print(f"íŒŒì¼: {result.metadata.file_name}")
    print(f"ìœ ì‚¬ë„: {result.similarity:.3f}")
    print(f"ë‚´ìš©: {result.text[:200]}...")

# í•„í„°ë§ ê²€ìƒ‰
pdf_results = rag.search('ê¸°ìˆ  ë³´ê³ ì„œ', extension_filter='.pdf')
recent_results = rag.search('AI', date_range=(yesterday, today))
""")

if __name__ == "__main__":
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    logging.getLogger().setLevel(logging.INFO)
    
    # ë°ëª¨ ì‹¤í–‰
    setup_and_run_qdrant_demo()
