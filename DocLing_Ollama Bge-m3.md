# N100 í™˜ê²½ìš© DocLing + Ollama bge-m3 RAG ì‹œìŠ¤í…œ ì„¤ì¹˜ ê°€ì´ë“œ

## ğŸ¯ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### ìµœì†Œ ìš”êµ¬ì‚¬í•­
- **CPU**: Intel N100 (4ì½”ì–´) ë˜ëŠ” ë™ê¸‰
- **RAM**: 8GB (16GB ê¶Œì¥)
- **ì €ì¥ê³µê°„**: 20GB ì—¬ìœ  ê³µê°„
- **OS**: Ubuntu 22.04+ / Debian 12+ / CentOS 8+

### ê¶Œì¥ ì‚¬ì–‘
- **RAM**: 16GB+
- **ì €ì¥ê³µê°„**: SSD ê¶Œì¥ (HDDëŠ” ì„±ëŠ¥ ì €í•˜ ì‹¬ê°)
- **ìŠ¤ì™‘**: 4GB+ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ)

## ğŸ“¦ 1ë‹¨ê³„: ê¸°ë³¸ ì‹œìŠ¤í…œ ì¤€ë¹„

### Python í™˜ê²½ ì„¤ì •
```bash
# Python 3.9+ ì„¤ì¹˜ í™•ì¸
python3 --version

# pip ì—…ê·¸ë ˆì´ë“œ
python3 -m pip install --upgrade pip

# ê°€ìƒ í™˜ê²½ ìƒì„± (ê¶Œì¥)
python3 -m venv rag_env
source rag_env/bin/activate  # Linux/Mac
# rag_env\Scripts\activate     # Windows
```

### ì‹œìŠ¤í…œ ìµœì í™”
```bash
# ìŠ¤ì™‘ ì„¤ì • (ë©”ëª¨ë¦¬ 8GB ì´í•˜ì¸ ê²½ìš°)
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# ì˜êµ¬ ì„¤ì •
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab

# VM ì„¤ì • ìµœì í™”
echo 'vm.swappiness=10' | sudo tee -a /etc/sysctl.conf
echo 'vm.vfs_cache_pressure=50' | sudo tee -a /etc/sysctl.conf
sudo sysctl -p
```

## ğŸ”§ 2ë‹¨ê³„: Ollama ì„¤ì¹˜ ë° ì„¤ì •

### Ollama ì„¤ì¹˜
```bash
# Ollama ì„¤ì¹˜
curl -fsSL https://ollama.ai/install.sh | sh

# ë˜ëŠ” ì§ì ‘ ë‹¤ìš´ë¡œë“œ
# wget https://github.com/ollama/ollama/releases/download/v0.1.17/ollama-linux-amd64
# sudo mv ollama-linux-amd64 /usr/local/bin/ollama
# sudo chmod +x /usr/local/bin/ollama
```

### N100 ìµœì í™” í™˜ê²½ë³€ìˆ˜ ì„¤ì •
```bash
# ~/.bashrc ë˜ëŠ” ~/.zshrcì— ì¶”ê°€
export OLLAMA_HOST=0.0.0.0:11434
export OLLAMA_NUM_PARALLEL=2           # N100 ì¿¼ë“œì½”ì–´ì˜ ì ˆë°˜
export OLLAMA_MAX_LOADED_MODELS=1      # ë©”ëª¨ë¦¬ ì ˆì•½
export OLLAMA_FLASH_ATTENTION=1        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
export OLLAMA_LLM_LIBRARY="cpu"        # CPU ì „ìš© ëª¨ë“œ

# ì ìš©
source ~/.bashrc
```

### Ollama ì„œë¹„ìŠ¤ ì‹œì‘
```bash
# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
nohup ollama serve > ollama.log 2>&1 &

# ë˜ëŠ” systemd ì„œë¹„ìŠ¤ë¡œ ë“±ë¡
sudo tee /etc/systemd/system/ollama.service > /dev/null <<EOF
[Unit]
Description=Ollama Service
After=network.target

[Service]
Type=simple
User=$USER
Environment=OLLAMA_HOST=0.0.0.0:11434
Environment=OLLAMA_NUM_PARALLEL=2
Environment=OLLAMA_MAX_LOADED_MODELS=1
ExecStart=/usr/local/bin/ollama serve
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl start ollama
```

### bge-m3 ëª¨ë¸ ì„¤ì¹˜
```bash
# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì•½ 2.5GB)
ollama pull bge-m3

# ì„¤ì¹˜ í™•ì¸
ollama list
```

## ğŸ 3ë‹¨ê³„: Python íŒ¨í‚¤ì§€ ì„¤ì¹˜

### í•µì‹¬ íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
# DocLing ì„¤ì¹˜
pip install docling

# ì¶”ê°€ ì˜ì¡´ì„±
pip install transformers torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# ê¸°íƒ€ í•„ìˆ˜ íŒ¨í‚¤ì§€
pip install numpy requests psutil aiohttp

# Intel ìµœì í™” (ì„ íƒì‚¬í•­)
pip install intel-extension-for-pytorch
```

### ì„¤ì¹˜ í™•ì¸
```python
# Pythonì—ì„œ ì‹¤í–‰
import docling
from transformers import AutoTokenizer
import torch
import requests

print("âœ… ëª¨ë“  íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ")
print(f"PyTorch ë²„ì „: {torch.__version__}")
print(f"DocLing ì‚¬ìš© ê°€ëŠ¥: {docling.__version__}")
```

## ğŸš€ 4ë‹¨ê³„: ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸

### Ollama ì—°ê²° í…ŒìŠ¤íŠ¸
```bash
# API í…ŒìŠ¤íŠ¸
curl http://localhost:11434/api/tags

# ì„ë² ë”© í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:11434/api/embeddings \
     -H "Content-Type: application/json" \
     -d '{"model": "bge-m3", "prompt": "Hello World"}'
```

### Python í†µí•© í…ŒìŠ¤íŠ¸
```python
# test_installation.py
import requests
import json

def test_ollama_bge():
    """Ollama bge-m3 í…ŒìŠ¤íŠ¸"""
    try:
        response = requests.post(
            "http://localhost:11434/api/embeddings",
            json={"model": "bge-m3", "prompt": "í…ŒìŠ¤íŠ¸ ë¬¸ì¥"},
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            embedding = data["embedding"]
            print(f"âœ… bge-m3 ì„ë² ë”© ì„±ê³µ: ì°¨ì› {len(embedding)}")
            return True
        else:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ ì—°ê²° ì˜¤ë¥˜: {e}")
        return False

if __name__ == "__main__":
    test_ollama_bge()
```

## ğŸ“Š 5ë‹¨ê³„: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„¤ì •

### ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸
```bash
#!/bin/bash
# monitor_n100.sh

echo "=== N100 RAG ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ==="
echo "ì‹œê°„: $(date)"
echo ""

# CPU ì‚¬ìš©ë¥ 
echo "ğŸ”§ CPU ì‚¬ìš©ë¥ :"
cat /proc/loadavg

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
echo ""
echo "ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:"
free -h

# Ollama í”„ë¡œì„¸ìŠ¤
echo ""
echo "ğŸ¤– Ollama ìƒíƒœ:"
ps aux | grep ollama | grep -v grep

# ì˜¨ë„ (lm-sensors í•„ìš”)
if command -v sensors &> /dev/null; then
    echo ""
    echo "ğŸŒ¡ï¸ CPU ì˜¨ë„:"
    sensors | grep Core
fi

# ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰
echo ""
echo "ğŸ’¿ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰:"
df -h / | tail -1

echo ""
echo "==================================="
```

### ìë™ ëª¨ë‹ˆí„°ë§ ì„¤ì •
```bash
# ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
chmod +x monitor_n100.sh

# cronìœ¼ë¡œ 5ë¶„ë§ˆë‹¤ ì‹¤í–‰
echo "*/5 * * * * /path/to/monitor_n100.sh >> /var/log/n100_monitor.log" | crontab -
```

## ğŸ® 6ë‹¨ê³„: RAG ì‹œìŠ¤í…œ ì‹¤í–‰

### ê¸°ë³¸ ì‹¤í–‰
```python
# run_rag.py
from complete_rag_docling_bge import setup_and_run_demo

if __name__ == "__main__":
    setup_and_run_demo()
```

### ì„œë¹„ìŠ¤ë¡œ ì‹¤í–‰
```bash
# rag_service.py ìƒì„±
python3 rag_service.py &

# í”„ë¡œì„¸ìŠ¤ í™•ì¸
ps aux | grep rag_service
```

## ğŸ” ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. Ollama ì—°ê²° ì‹¤íŒ¨
```bash
# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
systemctl status ollama

# ë¡œê·¸ í™•ì¸
journalctl -u ollama -f

# í¬íŠ¸ í™•ì¸
netstat -tlnp | grep 11434
```

#### 2. ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ìŠ¤ì™‘ ì‚¬ìš©ëŸ‰ í™•ì¸
swapon --show

# í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
ps aux --sort=-%mem | head -10

# ë©”ëª¨ë¦¬ ì •ë¦¬
echo 3 | sudo tee /proc/sys/vm/drop_caches
```

#### 3. ì„±ëŠ¥ ì €í•˜
```bash
# CPU ì£¼íŒŒìˆ˜ í™•ì¸
cat /proc/cpuinfo | grep MHz

# ì—´ì  ìŠ¤ë¡œí‹€ë§ í™•ì¸
dmesg | grep -i thermal

# I/O ëŒ€ê¸° í™•ì¸
iostat 1 5
```

#### 4. ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨
```bash
# ëª¨ë¸ ì¬ì„¤ì¹˜
ollama rm bge-m3
ollama pull bge-m3

# ìºì‹œ ì •ë¦¬
rm -rf ~/.ollama/models/blobs/*
```

## âš¡ N100 íŠ¹í™” ìµœì í™” íŒ

### 1. CPU ìµœì í™”
```bash
# CPU ê±°ë²„ë„ˆ ì„¤ì •
echo 'performance' | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor

# ì¸í„°ëŸ½íŠ¸ ë¶„ì‚°
echo 2 | sudo tee /proc/irq/*/smp_affinity
```

### 2. ë©”ëª¨ë¦¬ ìµœì í™”
```python
# Python ë©”ëª¨ë¦¬ ìµœì í™”
import gc
import os

# ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•í™”
gc.set_threshold(700, 10, 10)

# ë©”ëª¨ë¦¬ ë§¤í•‘ ìµœì í™”
os.environ['MALLOC_MMAP_THRESHOLD_'] = '65536'
os.environ['MALLOC_TRIM_THRESHOLD_'] = '131072'
```

### 3. ë„¤íŠ¸ì›Œí¬ ìµœì í™”
```bash
# TCP ìµœì í™”
echo 'net.core.rmem_default = 262144' | sudo tee -a /etc/sysctl.conf
echo 'net.core.rmem_max = 16777216' | sudo tee -a /etc/sysctl.conf
echo 'net.core.wmem_default = 262144' | sudo tee -a /etc/sysctl.conf
echo 'net.core.wmem_max = 16777216' | sudo tee -a /etc/sysctl.conf
```

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ì˜ˆìƒ ì„±ëŠ¥ ì§€í‘œ (N100 ê¸°ì¤€)
- **ë¬¸ì„œ ì²˜ë¦¬**: 1-3 í˜ì´ì§€/ë¶„
- **ì²­í‚¹ ì†ë„**: 100-300 ì²­í¬/ë¶„
- **ì„ë² ë”© ìƒì„±**: 5-15 ì²­í¬/ë¶„
- **ê²€ìƒ‰ ì§€ì—°ì‹œê°„**: 100-500ms
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 2-6GB
- **CPU ì‚¬ìš©ë¥ **: 60-90% (í”¼í¬ ì‹œ)

### ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
```python
# benchmark.py
from complete_rag_docling_bge import *
import time

def run_benchmark():
    config = RAGConfig(batch_size=1)
    rag = N100OptimizedRAG(config)
    
    # ë¬¸ì„œ ì¶”ê°€ ì„±ëŠ¥ ì¸¡ì •
    start = time.time()
    rag.add_document_from_text("test", "í…ŒìŠ¤íŠ¸ ë¬¸ì„œ" * 100)
    print(f"ë¬¸ì„œ ì¶”ê°€ ì‹œê°„: {time.time() - start:.2f}ì´ˆ")
    
    # ê²€ìƒ‰ ì„±ëŠ¥ ì¸¡ì •
    start = time.time()
    results = rag.search("í…ŒìŠ¤íŠ¸")
    print(f"ê²€ìƒ‰ ì‹œê°„: {time.time() - start:.2f}ì´ˆ")

if __name__ == "__main__":
    run_benchmark()
```

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **í”„ë¡œë•ì…˜ ë°°í¬**: Docker ì»¨í…Œì´ë„ˆ ë˜ëŠ” systemd ì„œë¹„ìŠ¤ë¡œ ì•ˆì •ì  ìš´ì˜
2. **ìŠ¤ì¼€ì¼ë§**: ë‹¤ì¤‘ ì¸ìŠ¤í„´ìŠ¤ ë˜ëŠ” ë¡œë“œ ë°¸ëŸ°ì‹± ê³ ë ¤
3. **ëª¨ë‹ˆí„°ë§**: Prometheus + Grafanaë¡œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
4. **ë°±ì—…**: ì„ë² ë”© ë°ì´í„°ë² ì´ìŠ¤ ì •ê¸° ë°±ì—… ì „ëµ
5. **ì—…ë°ì´íŠ¸**: ëª¨ë¸ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ ê´€ë¦¬

## ğŸ†˜ ì§€ì› ë° ì»¤ë®¤ë‹ˆí‹°

- **DocLing**: https://github.com/docling-project/docling
- **Ollama**: https://github.com/ollama/ollama
- **bge-m3**: https://huggingface.co/BAAI/bge-m3
- **ì´ìŠˆ ë¦¬í¬íŠ¸**: GitHub Issues ë˜ëŠ” ì»¤ë®¤ë‹ˆí‹° í¬ëŸ¼

---

âœ… **ì„¤ì¹˜ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸**
- [ ] Python 3.9+ ì„¤ì¹˜ ë° ê°€ìƒí™˜ê²½ ì„¤ì •
- [ ] Ollama ì„¤ì¹˜ ë° ì„œë¹„ìŠ¤ ë“±ë¡
- [ ] bge-m3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
- [ ] DocLing ë° ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜
- [ ] ì‹œìŠ¤í…œ ìµœì í™” ì„¤ì • ì ìš©
- [ ] ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ
- [ ] ë°ëª¨ ì‹¤í–‰ ì„±ê³µ
- [ ] ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì„¤ì •

ğŸ‰ **ì¶•í•˜í•©ë‹ˆë‹¤! N100 í™˜ê²½ìš© RAG ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.**
