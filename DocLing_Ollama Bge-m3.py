#!/usr/bin/env python3
"""
N100 CPU ìµœì í™”ëœ DocLing + Ollama bge-m3 RAG ì‹œìŠ¤í…œ
- ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì™„ì „í•œ êµ¬í˜„
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ ì„±ëŠ¥ ìµœì í™”ì— ì¤‘ì 
"""

import asyncio
import time
import logging
import psutil
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, field
from functools import lru_cache
import numpy as np
import requests
from concurrent.futures import ThreadPoolExecutor

# DocLing ì„í¬íŠ¸
try:
    from docling.document_converter import DocumentConverter
    from docling.chunking import HybridChunker
    from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer
    from transformers import AutoTokenizer
    DOCLING_AVAILABLE = True
except ImportError:
    print("âš ï¸ DocLingì„ ì„¤ì¹˜í•˜ì„¸ìš”: pip install docling")
    DOCLING_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class RAGConfig:
    """RAG ì‹œìŠ¤í…œ ì„¤ì •"""
    # Ollama ì„¤ì •
    ollama_base_url: str = "http://localhost:11434"
    embed_model: str = "bge-m3"
    
    # bge-m3 í† í¬ë‚˜ì´ì € ì„¤ì •
    bge_model_id: str = "BAAI/bge-m3"
    max_tokens: int = 512  # N100 ìµœì í™”
    
    # ì²­í‚¹ ì„¤ì •
    chunk_overlap: int = 50
    merge_peers: bool = True
    merge_list_items: bool = True
    
    # ì„±ëŠ¥ ì„¤ì •
    batch_size: int = 3
    max_parallel: int = 2
    timeout: int = 15
    cache_size: int = 1000
    
    # ê²€ìƒ‰ ì„¤ì •
    top_k: int = 5
    similarity_threshold: float = 0.3

@dataclass
class Document:
    """ë¬¸ì„œ í´ë˜ìŠ¤"""
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    file_path: Optional[str] = None

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ í´ë˜ìŠ¤"""
    text: str
    similarity: float
    metadata: Dict[str, Any]
    chunk_id: str

class OllamaEmbeddingClient:
    """Ollama ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, config: RAGConfig):
        self.config = config
        self.session = requests.Session()
        
        # ì—°ê²° í’€ ìµœì í™”
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=config.max_parallel,
            pool_maxsize=config.max_parallel * 2,
            max_retries=3
        )
        self.session.mount('http://', adapter)
        self.session.mount('https://', adapter)
        
        # ìºì‹œ
        self._embedding_cache = {}
        
    @lru_cache(maxsize=1000)
    def _cache_key(self, text: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return f"bge_m3_{hash(text[:100])}"  # ì²« 100ìë§Œ í•´ì‹œ
    
    def embed_text(self, text: str) -> Optional[np.ndarray]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        cache_key = self._cache_key(text)
        
        # ìºì‹œ í™•ì¸
        if cache_key in self._embedding_cache:
            return self._embedding_cache[cache_key]
        
        try:
            # í† í° ìˆ˜ ì œí•œ
            limited_text = text[:self.config.max_tokens * 4]  # ëŒ€ëµì  ê³„ì‚°
            
            response = self.session.post(
                f"{self.config.ollama_base_url}/api/embeddings",
                json={
                    "model": self.config.embed_model,
                    "prompt": limited_text
                },
                timeout=self.config.timeout
            )
            
            if response.status_code == 200:
                data = response.json()
                embedding = np.array(data["embedding"], dtype=np.float32)
                
                # ìºì‹œ ì €ì¥ (í¬ê¸° ì œí•œ)
                if len(self._embedding_cache) < self.config.cache_size:
                    self._embedding_cache[cache_key] = embedding
                
                return embedding
            else:
                logger.error(f"Ollama ì„ë² ë”© ì‹¤íŒ¨: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìš”ì²­ ì˜¤ë¥˜: {e}")
            return None
    
    def embed_batch(self, texts: List[str]) -> List[Optional[np.ndarray]]:
        """ë°°ì¹˜ ì„ë² ë”©"""
        with ThreadPoolExecutor(max_workers=self.config.max_parallel) as executor:
            futures = [executor.submit(self.embed_text, text) for text in texts]
            results = []
            
            for future in futures:
                try:
                    result = future.result(timeout=self.config.timeout)
                    results.append(result)
                except Exception as e:
                    logger.error(f"ë°°ì¹˜ ì„ë² ë”© ì˜¤ë¥˜: {e}")
                    results.append(None)
            
            return results
    
    def check_ollama_status(self) -> bool:
        """Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = self.session.get(
                f"{self.config.ollama_base_url}/api/tags", 
                timeout=5
            )
            
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [model['name'] for model in models]
                
                if self.config.embed_model in model_names:
                    logger.info(f"âœ… {self.config.embed_model} ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
                    return True
                else:
                    logger.warning(f"âŒ {self.config.embed_model} ëª¨ë¸ ì—†ìŒ")
                    logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {model_names}")
                    logger.info(f"ì„¤ì¹˜ ëª…ë ¹: ollama pull {self.config.embed_model}")
                    return False
            else:
                logger.error(f"Ollama API ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
            logger.info("Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: ollama serve")
            return False

class DoclingProcessor:
    """DocLing ë¬¸ì„œ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, config: RAGConfig):
        self.config = config
        
        if not DOCLING_AVAILABLE:
            raise ImportError("DocLingì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        try:
            tokenizer = HuggingFaceTokenizer(
                tokenizer=AutoTokenizer.from_pretrained(config.bge_model_id),
                max_tokens=config.max_tokens
            )
            
            # HybridChunker ì´ˆê¸°í™”
            self.chunker = HybridChunker(
                tokenizer=tokenizer,
                merge_peers=config.merge_peers,
                merge_list_items=config.merge_list_items
            )
            
            # ë¬¸ì„œ ë³€í™˜ê¸°
            self.converter = DocumentConverter()
            
            logger.info("âœ… DocLing í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"DocLing ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def process_file(self, file_path: str) -> List[str]:
        """íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ì»¨í…ìŠ¤íŠ¸í™”ëœ ì²­í¬ ë°˜í™˜"""
        try:
            # ë¬¸ì„œ ë³€í™˜
            logger.info(f"ğŸ“„ íŒŒì¼ ì²˜ë¦¬ ì¤‘: {file_path}")
            converted_doc = self.converter.convert(source=file_path)
            
            # ì²­í‚¹ ë° ì»¨í…ìŠ¤íŠ¸í™”
            chunks = []
            for chunk in self.chunker.chunk(dl_doc=converted_doc.document):
                # contextualize ì ìš© (ì‹¤ì œë¡œëŠ” serialize)
                contextualized_text = self.chunker.serialize(chunk)
                chunks.append(contextualized_text)
            
            logger.info(f"âœ… {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
            return chunks
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
            return []
    
    def process_text(self, text: str) -> List[str]:
        """ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê³  ì²­í¬ ë°˜í™˜"""
        try:
            # ì„ì‹œ ë¬¸ì„œ ê°ì²´ ìƒì„± (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ì •êµí•´ì•¼ í•¨)
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ë¬¸ë‹¨ ë¶„í•  ì‚¬ìš©
            paragraphs = text.split('\n\n')
            chunks = []
            
            for para in paragraphs:
                if len(para.strip()) > 0:
                    # ê¸´ ë¬¸ë‹¨ì€ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
                    if len(para) > self.config.max_tokens * 3:
                        sentences = para.split('. ')
                        current_chunk = ""
                        
                        for sentence in sentences:
                            if len(current_chunk) + len(sentence) < self.config.max_tokens * 3:
                                current_chunk += sentence + ". "
                            else:
                                if current_chunk:
                                    chunks.append(current_chunk.strip())
                                current_chunk = sentence + ". "
                        
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                    else:
                        chunks.append(para.strip())
            
            return chunks
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return []

class N100OptimizedRAG:
    """N100 ìµœì í™”ëœ ì™„ì „í•œ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: RAGConfig):
        self.config = config
        
        # êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self.embedding_client = OllamaEmbeddingClient(config)
        
        if DOCLING_AVAILABLE:
            self.docling_processor = DoclingProcessor(config)
        else:
            self.docling_processor = None
            logger.warning("DocLing ë¹„í™œì„±í™” - í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë“œ")
        
        # ë¬¸ì„œ ì €ì¥ì†Œ
        self.documents: Dict[str, Document] = {}
        self.chunk_embeddings: Dict[str, Dict[str, Any]] = {}
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.performance_stats = {
            'documents_processed': 0,
            'chunks_created': 0,
            'searches_performed': 0,
            'avg_search_time': 0.0,
            'cache_hits': 0,
            'cache_misses': 0
        }
    
    def add_document_from_file(self, file_path: str, metadata: Optional[Dict] = None) -> bool:
        """íŒŒì¼ì—ì„œ ë¬¸ì„œ ì¶”ê°€"""
        if not self.docling_processor:
            logger.error("DocLing í”„ë¡œì„¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        try:
            # íŒŒì¼ ì²˜ë¦¬
            chunks = self.docling_processor.process_file(file_path)
            
            if not chunks:
                logger.warning(f"íŒŒì¼ì—ì„œ ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
                return False
            
            # ë¬¸ì„œ ì¶”ê°€
            doc = Document(
                content="\n\n".join(chunks),
                metadata=metadata or {},
                file_path=file_path
            )
            
            return self._add_document_chunks(file_path, chunks, doc.metadata)
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def add_document_from_text(self, doc_id: str, text: str, metadata: Optional[Dict] = None) -> bool:
        """í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì„œ ì¶”ê°€"""
        try:
            # í…ìŠ¤íŠ¸ ì²˜ë¦¬
            if self.docling_processor:
                chunks = self.docling_processor.process_text(text)
            else:
                # ê°„ë‹¨í•œ ì²­í‚¹
                chunks = self._simple_chunk_text(text)
            
            if not chunks:
                logger.warning(f"í…ìŠ¤íŠ¸ì—ì„œ ì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_id}")
                return False
            
            # ë¬¸ì„œ ì¶”ê°€
            doc = Document(
                content=text,
                metadata=metadata or {}
            )
            
            return self._add_document_chunks(doc_id, chunks, doc.metadata)
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def _simple_chunk_text(self, text: str) -> List[str]:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì²­í‚¹ (DocLing ì—†ì„ ë•Œ)"""
        max_chars = self.config.max_tokens * 3  # ëŒ€ëµì  ê³„ì‚°
        
        if len(text) <= max_chars:
            return [text]
        
        chunks = []
        words = text.split()
        current_chunk = []
        current_length = 0
        
        for word in words:
            if current_length + len(word) > max_chars and current_chunk:
                chunks.append(" ".join(current_chunk))
                current_chunk = [word]
                current_length = len(word)
            else:
                current_chunk.append(word)
                current_length += len(word) + 1
        
        if current_chunk:
            chunks.append(" ".join(current_chunk))
        
        return chunks
    
    def _add_document_chunks(self, doc_id: str, chunks: List[str], metadata: Dict) -> bool:
        """ì²­í¬ë“¤ì„ ì„ë² ë”©í•˜ê³  ì €ì¥"""
        try:
            logger.info(f"ğŸ“ {len(chunks)}ê°œ ì²­í¬ ì„ë² ë”© ì¤‘...")
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”©
            all_embeddings = []
            
            for i in range(0, len(chunks), self.config.batch_size):
                batch = chunks[i:i + self.config.batch_size]
                batch_embeddings = self.embedding_client.embed_batch(batch)
                all_embeddings.extend(batch_embeddings)
                
                # N100 ì—´ ê´€ë¦¬
                time.sleep(0.3)
                
                progress = min(i + self.config.batch_size, len(chunks))
                logger.info(f"   ì§„í–‰ë¥ : {progress}/{len(chunks)}")
            
            # ê²°ê³¼ ì €ì¥
            successful_chunks = 0
            for i, (chunk, embedding) in enumerate(zip(chunks, all_embeddings)):
                if embedding is not None:
                    chunk_id = f"{doc_id}_chunk_{i}"
                    
                    self.chunk_embeddings[chunk_id] = {
                        'embedding': embedding,
                        'text': chunk,
                        'metadata': {
                            **metadata,
                            'doc_id': doc_id,
                            'chunk_index': i,
                            'chunk_id': chunk_id
                        }
                    }
                    successful_chunks += 1
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.performance_stats['documents_processed'] += 1
            self.performance_stats['chunks_created'] += successful_chunks
            
            logger.info(f"âœ… {successful_chunks}/{len(chunks)} ì²­í¬ ì„±ê³µì ìœ¼ë¡œ ì¶”ê°€")
            return successful_chunks > 0
            
        except Exception as e:
            logger.error(f"ì²­í¬ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            return False
    
    def search(self, query: str, top_k: Optional[int] = None) -> List[SearchResult]:
        """ë¬¸ì„œ ê²€ìƒ‰"""
        start_time = time.time()
        top_k = top_k or self.config.top_k
        
        try:
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.embedding_client.embed_text(query)
            
            if query_embedding is None:
                logger.error("ì¿¼ë¦¬ ì„ë² ë”© ì‹¤íŒ¨")
                return []
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            similarities = []
            for chunk_id, chunk_data in self.chunk_embeddings.items():
                similarity = self._cosine_similarity(
                    query_embedding, 
                    chunk_data['embedding']
                )
                
                # ì„ê³„ê°’ í•„í„°ë§
                if similarity >= self.config.similarity_threshold:
                    similarities.append({
                        'chunk_id': chunk_id,
                        'similarity': similarity,
                        'text': chunk_data['text'],
                        'metadata': chunk_data['metadata']
                    })
            
            # ìƒìœ„ kê°œ ì •ë ¬
            similarities.sort(key=lambda x: x['similarity'], reverse=True)
            top_results = similarities[:top_k]
            
            # SearchResult ê°ì²´ë¡œ ë³€í™˜
            results = [
                SearchResult(
                    text=result['text'],
                    similarity=result['similarity'],
                    metadata=result['metadata'],
                    chunk_id=result['chunk_id']
                ) for result in top_results
            ]
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            search_time = time.time() - start_time
            self.performance_stats['searches_performed'] += 1
            
            current_avg = self.performance_stats['avg_search_time']
            searches_count = self.performance_stats['searches_performed']
            self.performance_stats['avg_search_time'] = (
                (current_avg * (searches_count - 1) + search_time) / searches_count
            )
            
            logger.info(f"ğŸ” ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼, {search_time:.3f}ì´ˆ")
            return results
            
        except Exception as e:
            logger.error(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            return float(dot_product / (norm1 * norm2))
        except Exception:
            return 0.0
    
    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´"""
        memory = psutil.virtual_memory()
        cpu_percent = psutil.cpu_percent(interval=1)
        
        return {
            'documents_count': self.performance_stats['documents_processed'],
            'chunks_count': self.performance_stats['chunks_created'],
            'searches_performed': self.performance_stats['searches_performed'],
            'avg_search_time': self.performance_stats['avg_search_time'],
            'cache_size': len(self.embedding_client._embedding_cache),
            'memory_usage_percent': memory.percent,
            'cpu_usage_percent': cpu_percent,
            'memory_available_gb': memory.available / (1024**3),
            'ollama_connected': self.embedding_client.check_ollama_status()
        }
    
    def optimize_for_n100(self):
        """N100 í™˜ê²½ ìµœì í™” ì‹¤í–‰"""
        logger.info("ğŸ”§ N100 í™˜ê²½ ìµœì í™” ì ìš© ì¤‘...")
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        import gc
        gc.collect()
        
        # ìºì‹œ í¬ê¸° ì¡°ì •
        cache_size = len(self.embedding_client._embedding_cache)
        if cache_size > self.config.cache_size * 0.8:
            # ìºì‹œ ì •ë¦¬ (LRU ë°©ì‹)
            cache_items = list(self.embedding_client._embedding_cache.items())
            keep_size = self.config.cache_size // 2
            self.embedding_client._embedding_cache = dict(cache_items[-keep_size:])
            logger.info(f"ğŸ§¹ ìºì‹œ ì •ë¦¬: {cache_size} â†’ {keep_size}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
        memory = psutil.virtual_memory()
        if memory.percent > 85:
            logger.warning(f"âš ï¸ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory.percent:.1f}%")
        
        logger.info("âœ… N100 ìµœì í™” ì™„ë£Œ")

# ì‹¤ì œ ì‚¬ìš© ì˜ˆì œì™€ í…ŒìŠ¤íŠ¸
class RAGSystemTester:
    """RAG ì‹œìŠ¤í…œ í…ŒìŠ¤í„°"""
    
    def __init__(self, rag_system: N100OptimizedRAG):
        self.rag = rag_system
    
    def run_comprehensive_test(self):
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ§ª RAG ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # 1. Ollama ì—°ê²° í…ŒìŠ¤íŠ¸
        self._test_ollama_connection()
        
        # 2. ë¬¸ì„œ ì¶”ê°€ í…ŒìŠ¤íŠ¸
        self._test_document_addition()
        
        # 3. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        self._test_search_functionality()
        
        # 4. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        self._test_performance()
        
        # 5. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        self._check_system_status()
    
    def _test_ollama_connection(self):
        """Ollama ì—°ê²° í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ”Œ Ollama ì—°ê²° í…ŒìŠ¤íŠ¸...")
        
        if self.rag.embedding_client.check_ollama_status():
            logger.info("âœ… Ollama ì—°ê²° ì„±ê³µ")
        else:
            logger.error("âŒ Ollama ì—°ê²° ì‹¤íŒ¨")
            return False
        
        # ê°„ë‹¨í•œ ì„ë² ë”© í…ŒìŠ¤íŠ¸
        test_embedding = self.rag.embedding_client.embed_text("í…ŒìŠ¤íŠ¸")
        if test_embedding is not None:
            logger.info(f"âœ… ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì„±ê³µ (ì°¨ì›: {len(test_embedding)})")
        else:
            logger.error("âŒ ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        
        return True
    
    def _test_document_addition(self):
        """ë¬¸ì„œ ì¶”ê°€ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ“š ë¬¸ì„œ ì¶”ê°€ í…ŒìŠ¤íŠ¸...")
        
        # ìƒ˜í”Œ ë¬¸ì„œë“¤
        test_documents = [
            {
                'id': 'doc1',
                'text': "IBMì€ 1911ë…„ì— ì„¤ë¦½ëœ ë¯¸êµ­ì˜ ë‹¤êµ­ì  ê¸°ìˆ  íšŒì‚¬ì…ë‹ˆë‹¤. ì¸ê³µì§€ëŠ¥, í´ë¼ìš°ë“œ ì»´í“¨íŒ…, ì–‘ì ì»´í“¨íŒ… ë¶„ì•¼ì—ì„œ ì„ ë„ì ì¸ ì—­í• ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. 1960ë…„ëŒ€ë¶€í„° ë©”ì¸í”„ë ˆì„ ì»´í“¨í„° ì‹œì¥ì„ ì£¼ë„í–ˆìœ¼ë©°, System/360ìœ¼ë¡œ ì»´í“¨í„° ì‚°ì—…ì„ í˜ì‹ í–ˆìŠµë‹ˆë‹¤.",
                'metadata': {'source': 'wiki', 'topic': 'IBM', 'language': 'ko'}
            },
            {
                'id': 'doc2', 
                'text': "DocLingì€ IBM Researchì—ì„œ ê°œë°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ë¬¸ì„œ ë³€í™˜ ë„êµ¬ì…ë‹ˆë‹¤. PDF, DOCX, PPTX ë“± ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë¬¸ì„œë¥¼ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. HybridChunkerë¥¼ í†µí•´ ë¬¸ë§¥ì„ ë³´ì¡´í•˜ëŠ” ì²­í‚¹ ê¸°ëŠ¥ì„ ì œê³µí•˜ë©°, contextualize ë©”ì†Œë“œë¡œ ê° ì²­í¬ì— ë¬¸ì„œì˜ ê³„ì¸µì  êµ¬ì¡° ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.",
                'metadata': {'source': 'docs', 'topic': 'DocLing', 'language': 'ko'}
            },
            {
                'id': 'doc3',
                'text': "bge-m3ëŠ” BAAI(Beijing Academy of Artificial Intelligence)ì—ì„œ ê°œë°œí•œ ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ì…ë‹ˆë‹¤. í•œêµ­ì–´, ì˜ì–´, ì¤‘êµ­ì–´ ë“± 100ì—¬ ê°œ ì–¸ì–´ë¥¼ ì§€ì›í•˜ë©°, ìµœëŒ€ 8192 í† í°ê¹Œì§€ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. MTEB(Massive Text Embedding Benchmark)ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, íŠ¹íˆ ë‹¤êµ­ì–´ ê²€ìƒ‰ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ê²°ê³¼ë¥¼ ë³´ì…ë‹ˆë‹¤.",
                'metadata': {'source': 'research', 'topic': 'bge-m3', 'language': 'ko'}
            }
        ]
        
        success_count = 0
        for doc in test_documents:
            if self.rag.add_document_from_text(doc['id'], doc['text'], doc['metadata']):
                success_count += 1
                logger.info(f"âœ… ë¬¸ì„œ ì¶”ê°€ ì„±ê³µ: {doc['id']}")
            else:
                logger.error(f"âŒ ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {doc['id']}")
        
        logger.info(f"ğŸ“Š ë¬¸ì„œ ì¶”ê°€ ê²°ê³¼: {success_count}/{len(test_documents)}")
        return success_count == len(test_documents)
    
    def _test_search_functionality(self):
        """ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸ” ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        test_queries = [
            "IBMì˜ ì—­ì‚¬ì™€ System/360ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
            "DocLingì˜ HybridChunkerì™€ contextualize ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "bge-m3 ëª¨ë¸ì˜ íŠ¹ì§•ê³¼ ì§€ì› ì–¸ì–´ëŠ”?",
            "ì–‘ì ì»´í“¨íŒ… ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
            "ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ì–´ë–¤ê°€ìš”?"
        ]
        
        for i, query in enumerate(test_queries, 1):
            logger.info(f"\nğŸ” ì¿¼ë¦¬ {i}: {query}")
            
            results = self.rag.search(query, top_k=3)
            
            if results:
                logger.info(f"ğŸ“„ {len(results)}ê°œ ê²°ê³¼ ë°œê²¬:")
                for j, result in enumerate(results, 1):
                    logger.info(f"  {j}. ìœ ì‚¬ë„: {result.similarity:.3f}")
                    logger.info(f"     ë‚´ìš©: {result.text[:100]}...")
                    logger.info(f"     ì£¼ì œ: {result.metadata.get('topic', 'N/A')}")
            else:
                logger.warning("âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
    
    def _test_performance(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("âš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
        
        # ë°˜ë³µ ê²€ìƒ‰ìœ¼ë¡œ ìºì‹œ íš¨ê³¼ ì¸¡ì •
        test_query = "IBM ê¸°ìˆ "
        times = []
        
        for i in range(5):
            start_time = time.time()
            results = self.rag.search(test_query)
            search_time = time.time() - start_time
            times.append(search_time)
            logger.info(f"  ê²€ìƒ‰ {i+1}: {search_time:.3f}ì´ˆ")
        
        avg_time = sum(times) / len(times)
        logger.info(f"ğŸ“Š í‰ê·  ê²€ìƒ‰ ì‹œê°„: {avg_time:.3f}ì´ˆ")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory = psutil.virtual_memory()
        logger.info(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {memory.percent:.1f}%")
        
        # N100 ìµœì í™” ì‹¤í–‰
        self.rag.optimize_for_n100()
    
    def _check_system_status(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        logger.info("ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸...")
        
        status = self.rag.get_system_status()
        
        logger.info(f"ğŸ“ˆ ì‹œìŠ¤í…œ í†µê³„:")
        logger.info(f"  - ì²˜ë¦¬ëœ ë¬¸ì„œ: {status['documents_count']}")
        logger.info(f"  - ìƒì„±ëœ ì²­í¬: {status['chunks_count']}")
        logger.info(f"  - ìˆ˜í–‰ëœ ê²€ìƒ‰: {status['searches_performed']}")
        logger.info(f"  - í‰ê·  ê²€ìƒ‰ ì‹œê°„: {status['avg_search_time']:.3f}ì´ˆ")
        logger.info(f"  - ìºì‹œ í¬ê¸°: {status['cache_size']}")
        logger.info(f"  - ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {status['memory_usage_percent']:.1f}%")
        logger.info(f"  - CPU ì‚¬ìš©ë¥ : {status['cpu_usage_percent']:.1f}%")
        logger.info(f"  - ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {status['memory_available_gb']:.1f}GB")

def setup_and_run_demo():
    """ë°ëª¨ ì„¤ì • ë° ì‹¤í–‰"""
    print("ğŸš€ N100 + DocLing + Ollama bge-m3 RAG ì‹œìŠ¤í…œ ë°ëª¨")
    print("=" * 60)
    
    # ì„¤ì •
    config = RAGConfig(
        max_tokens=512,      # N100ìš© ìµœì í™”
        batch_size=3,        # ì‘ì€ ë°°ì¹˜
        max_parallel=2,      # ì¿¼ë“œì½”ì–´ì˜ ì ˆë°˜
        cache_size=500,      # ë©”ëª¨ë¦¬ ì ˆì•½
        top_k=5
    )
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    try:
        rag_system = N100OptimizedRAG(config)
        logger.info("âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # í…ŒìŠ¤í„° ì‹¤í–‰
    tester = RAGSystemTester(rag_system)
    tester.run_comprehensive_test()
    
    print("\n" + "=" * 60)
    print("ğŸ¯ N100 í™˜ê²½ ìµœì í™” íŒ:")
    print("1. Ollama ì„¤ì •: export OLLAMA_NUM_PARALLEL=2")
    print("2. ëª¨ë¸ ì„¤ì¹˜: ollama pull bge-m3")
    print("3. ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§: watch -n 1 'free -h'")
    print("4. ì˜¨ë„ í™•ì¸: sensors (lm-sensors íŒ¨í‚¤ì§€)")
    print("5. ìŠ¤ì™‘ ì„¤ì •: sudo swapon /swapfile (í•„ìš”ì‹œ)")

if __name__ == "__main__":
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    logging.getLogger().setLevel(logging.INFO)
    
    # ë°ëª¨ ì‹¤í–‰
    setup_and_run_demo()
